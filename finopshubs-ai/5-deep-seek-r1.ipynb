{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
        "\n",
        "**DeepSeek-R1** is a state-of-the-art reasoning model combining reinforcement learning and supervised fine-tuning, excelling at complex reasoning tasks with 37B active parameters and 128K context window.\n",
        "\n",
        "In this notebook, you'll learn to:\n",
        "1. **Initialize** the ChatCompletionsClient for Azure serverless endpoints\n",
        "2. **Chat** with DeepSeek-R1 using reasoning extraction\n",
        "3. **Implement** a travel planning example with step-by-step reasoning\n",
        "4. **Leverage** the 128K context window for complex scenarios\n",
        "\n",
        "## Why DeepSeek-R1?\n",
        "- **Advanced Reasoning**: Specializes in chain-of-thought problem solving\n",
        "- **Massive Context**: 128K token window for detailed analysis\n",
        "- **Efficient Architecture**: 37B active parameters from 671B total\n",
        "- **Safety Integrated**: Built-in content filtering capabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication\n",
        "\n",
        "Required packages:\n",
        "- `azure-ai-inference`: For chat completions\n",
        "- `python-dotenv`: For environment variables\n",
        "\n",
        ".env file requirements:\n",
        "```bash\n",
        "AZURE_INFERENCE_ENDPOINT=<your-endpoint-url>\n",
        "AZURE_INFERENCE_KEY=<your-api-key>\n",
        "MODEL_NAME=DeepSeek-R1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Client initialized | Model: deepseek-r1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "model_name = os.getenv(\"MODEL_NAMEDS\", \"DeepSeek-R1\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key),\n",
        "        headers={\"x-ms-model-mesh-model-name\": model_name}  # Add the model name in the header\n",
        "    )\n",
        "    print(\"‚úÖ Client initialized | Model:\", client.get_model_info().model_name)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Initialization failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. Technical Problem Solving üíª\n",
        "\n",
        "Showcase coding/optimization capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí∞ FinOps Challenge: As a FinOps specialist, analyze how to optimize costs for an Azure environment with 50 VMs (mix of D-series and F-series) \n",
            "running in West Europe. Most VMs are running 24/7, but actual usage patterns show 60% utilization during business hours and 15% during nights/weekends. \n",
            "What cost-saving strategies would you recommend, including newer VM generations that might offer better performance/cost ratio?\n",
            "\n",
            "üí° Optimization Strategy: <think>\n",
            "Okay, let's tackle this Azure cost optimization problem. The user has 50 VMs, a mix of D and F series in West Europe, running 24/7. Their utilization is 60% during business hours and drops to 15% at nights and weekends. I need to figure out cost-saving strategies, maybe suggest newer VM generations.\n",
            "\n",
            "First, let me recall Azure VM types. D-series are general-purpose, F-series are compute optimized. Both have newer versions like Dv5, Dasv5, Fsv2, etc. Newer generations usually offer better performance per dollar, so upgrading might save costs even if the specs look similar.\n",
            "\n",
            "Right-sizing is key. Since utilization is 60% during the day and 15% off-hours, maybe the VMs are over-provisioned. Azure Advisor can help identify underutilized VMs. Maybe some can be downsized. For example, if a VM is using 60% CPU on a D4, maybe a D3 would suffice. But need to check memory and disk too.\n",
            "\n",
            "Reserved Instances (RIs) are a must for 24/7 workloads. They offer up to 72% savings over pay-as-you-go. Since most VMs run 24/7, buying 1 or 3-year RIs for the baseline load would help. But what about the variable part? Maybe use RIs for the base and handle peaks with on-demand or spot.\n",
            "\n",
            "Wait, but the usage drops at night. If they're running 24/7, but utilization is low, maybe shutting down some VMs during off-hours. However, the user says \"most VMs are running 24/7\", so maybe not all can be stopped. But if some can be turned off, that's a saving. Azure Automation or VMSS with schedules could automate start/stop.\n",
            "\n",
            "Spot VMs for non-critical workloads. If some VMs can handle interruptions, like batch jobs or dev/test, use spot instances for up to 90% savings. But since these are production VMs, maybe not all can use spot. Need to check which workloads are suitable.\n",
            "\n",
            "Hybrid Benefit is another angle. If they have Windows Server or SQL Server licenses with Software Assurance, they can save on licensing costs by bringing their own licenses. That's up to 49% savings on Windows VMs.\n",
            "\n",
            "Azure Savings Plan. It's a commitment to spend a certain amount hourly for 1 or 3 years, offering savings up to 65%. It's more flexible than RIs because it applies across VM families and regions. Maybe combine with RIs for maximum savings.\n",
            "\n",
            "Now, newer VM generations. Dv5 and Dasv5 use Intel and AMD respectively, better price/performance. Fsv2 and Fsv4 are newer compute-optimized. For example, Fsv2 has a higher Azure Compute Unit (ACU) than F-series. So migrating to these could allow using smaller instances for same performance, saving costs.\n",
            "\n",
            "Storage optimization. Maybe using Premium SSD v2 or Standard SSDs instead of Premium if not needed. Also, managed disks can be resized without downtime, so adjusting disk sizes based on need.\n",
            "\n",
            "Monitoring and governance. Azure Cost Management tools, budgets, alerts. Tagging resources to track costs by department or project. Maybe implement policies to enforce shutdown schedules.\n",
            "\n",
            "Putting it all together: First, right-size VMs using Azure Advisor. Then, for the 24/7 baseline, buy RIs or Savings Plan. For the variable part, use autoscaling or schedule shutdowns. Upgrade to newer VM series. Use Hybrid Benefit if applicable. Implement automation for start/stop. Maybe use spot for non-critical parts. Also check storage and networking costs, but the user didn't mention those, so focus on compute.\n",
            "\n",
            "Key recommendations would be:\n",
            "\n",
            "1. Right-size underutilized VMs.\n",
            "2. Use Reserved Instances for baseline 24/7 workloads.\n",
            "3. Implement auto-shutdown for non-peak hours.\n",
            "4. Migrate to newer VM generations (Dv5, Dasv5, Fsv2).\n",
            "5. Leverage Azure Hybrid Benefit.\n",
            "6. Consider Azure Savings Plan for flexible commitment.\n",
            "7. Explore Spot VMs for eligible workloads.\n",
            "8. Monitor and optimize continuously.\n",
            "\n",
            "Need to check if all VMs are necessary 24/7. Maybe some can be scaled in or out. Also, check if Azure Automanage can help with optimization. Maybe using VMSS for some workloads to scale automatically. But with 50 VMs, maybe a mix of strategies. Also, check region-specific pricing in West Europe for the recommended instances.\n",
            "</think>\n",
            "\n",
            "Here's a structured cost optimization strategy for your Azure environment, combining immediate actions and longer-term architectural improvements:\n",
            "\n",
            "**1. Right-Sizing & Modern Instance Types (Immediate Impact)**\n",
            "- *Current Analysis*: 60% daytime utilization suggests over-provisioning. Use Azure Advisor's right-sizing recommendations and Perf Metrics (CPU/Memory/Disk) to identify candidates for downsizing.\n",
            "  \n",
            "- *Modern VM Recommendations*:\n",
            "  - **Dv5/Dasv5 Series**: Replace D-series with 3rd Gen AMD EPYC‚Ñ¢ (Dasv5) or Intel¬Æ Xeon¬Æ (Dv5). Offers 20-25% better price/performance.\n",
            "  - **Fsv2/Fsv4 Series**: Replace F-series with AMD EPYC‚Ñ¢ (Fsv2) or Intel¬Æ (Fsv4). Up to 35% better compute/$.\n",
            "  - *Example Migration*: F16s_v2 (16 vCPU) often matches F32s performance at half the cost.\n",
            "\n",
            "**2. Schedule Optimization (20-40% Savings)**\n",
            "- *Auto-Shutdown*: For non-essential VMs, implement Azure Automation or DevTest Labs schedules:\n",
            "  - Night shutdown (e.g., 8 PM - 7 AM)\n",
            "  - Weekend shutdown (Sat/Sun)\n",
            "- *Scale Sets*: For stateless workloads, use VMSS with auto-scaling rules based on CPU thresholds.\n",
            "\n",
            "**3. Purchasing Strategy (40-72% Savings)**\n",
            "- *Reserved Instances*: Commit to 1-year reservations for 24/7 core VMs (calculate baseline capacity needed after right-sizing).\n",
            "- *Savings Plan*: For remaining variable usage, commit to $10k-$15k/month (estimate) for 1-year to cover fluctuating compute.\n",
            "- *Spot Instances*: Use for batch processing, CI/CD, or non-critical workloads (up to 90% discount).\n",
            "\n",
            "**4. License Optimization (Up to 49% Savings)**\n",
            "- Apply Azure Hybrid Benefit to all Windows Server VMs (requires SA licenses).\n",
            "- Check SQL Server AHB eligibility if applicable.\n",
            "\n",
            "**5. Storage Optimization**\n",
            "- Convert Premium SSDs to Standard SSDs for non-IOPs sensitive workloads.\n",
            "- Implement Azure Disk Storage bursting for temporary load spikes.\n",
            "\n",
            "**6. Monitoring & Governance**\n",
            "- Enable Cost Anomaly Detection\n",
            "- Set budget alerts at 75%/90%/100% thresholds\n",
            "- Implement resource tagging for cost allocation\n",
            "\n",
            "**Example Savings Calculation**:\n",
            "For 50 VMs (assume $400/month avg per VM):\n",
            "- Baseline: $20,000/mo\n",
            "- After right-sizing + modern instances: $16,000 (-20%)\n",
            "- Reservations: $11,200 (-30% of remaining)\n",
            "- Schedule shutdowns (33% time reduction): $7,500 (-33%)\n",
            "- Total Potential Savings: **62.5%** ($7,500 vs $20,000)\n",
            "\n",
            "**Implementation Roadmap**:\n",
            "1. Run Azure Advisor recommendations (1 week)\n",
            "2. Pilot modern instances with 20% of fleet (2 weeks)\n",
            "3. Implement shutdown schedules (1 week)\n",
            "4. Purchase reservations/savings plan (Month 2)\n",
            "5. Full migration to spot for eligible workloads (Month 3)\n",
            "\n",
            "**Key Risk Mitigation**:\n",
            "- Test modern VMs with mirrored workloads\n",
            "- Implement load testing before downsizing\n",
            "- Use Azure Site Recovery for migration safety\n",
            "\n",
            "Would you like me to provide specific CLI commands for implementing any of these recommendations or create a detailed migration plan for a subset of VMs?\n"
          ]
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=\"You are a FinOps expert specializing in Azure cost optimization. Provide detailed, actionable advice.\"),\n",
        "            UserMessage(content=f\"{problem} Please reason step by step and highlight key recommendations.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# FinOps for Azure optimization examples\n",
        "problem = \"\"\"As a FinOps specialist, analyze how to optimize costs for an Azure environment with 50 VMs (mix of D-series and F-series) \n",
        "running in West Europe. Most VMs are running 24/7, but actual usage patterns show 60% utilization during business hours and 15% during nights/weekends. \n",
        "What cost-saving strategies would you recommend, including newer VM generations that might offer better performance/cost ratio?\"\"\"\n",
        "\n",
        "print(\"üí∞ FinOps Challenge:\", problem)\n",
        "print(\"\\nüí° Optimization Strategy:\", solve_technical_problem(problem))\n",
        "\n",
        "# Additional FinOps examples (commented for future use)\n",
        "# \"\"\"\n",
        "# # Example 2: Reserved Instance Analysis\n",
        "problem_ri = '''Analyze the cost-benefit of committing to 3-year reserved instances for 20 Standard_D4s_v3 VMs \n",
        "currently on pay-as-you-go pricing in West Europe. Include ROI calculation and break-even analysis.'''\n",
        "\n",
        "# # Example 3: VM Right-sizing\n",
        "# problem_rightsize = '''Our monitoring shows our Standard_F8s_v2 VMs are consistently using less than 30% CPU and 40% memory.\n",
        "# Recommend right-sizing options that maintain the same number of NICs but optimize costs.'''\n",
        "\n",
        "# # Example 4: Hybrid Azure/On-Premises Strategy\n",
        "# problem_hybrid = '''We currently run 35 VMs in Azure and 40 on-premises servers nearing end-of-life.\n",
        "# What would be the most cost-effective migration strategy considering TCO for the next 5 years?'''\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "19ed59dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß FinOps Challenge: Our Power BI reports connected to our FinOps hub storage aren't showing the latest data. \n",
            "We're using a service principal to connect to storage. What should we verify to ensure proper data refresh?\n",
            "\n",
            "üí° Solution: <think>\n",
            "Okay, let's tackle this problem. The user has deployed a FinOps hub but is encountering errors in their Data Factory pipelines, which is preventing Cost Management exports from flowing into their storage account for Power BI reporting. I need to figure out the possible causes and provide a step-by-step troubleshooting guide.\n",
            "\n",
            "First, I'll recall the components involved. The FinOps hub setup likely uses Azure Data Factory (ADF) pipelines to move cost data from Cost Management exports into a storage account, which Power BI then uses for reporting. Errors in ADF could be due to various issues like permissions, configuration errors, data format problems, or service availability.\n",
            "\n",
            "Starting with the storage account, I should check if the storage is accessible and properly configured. Maybe the storage account's firewall is blocking access, or the ADF managed identity doesn't have the right RBAC roles (like Storage Blob Data Contributor). Also, checking if the container and blob paths are correctly specified in the pipeline.\n",
            "\n",
            "Next, looking at the Cost Management exports. They need to be correctly set up to export data to the storage account. The export job might have failed, or there's a delay in data generation. Verifying the export history in the Azure portal would help here. Also, ensuring the export's storage location matches where ADF is trying to read from.\n",
            "\n",
            "For the Data Factory pipelines, the user should check the pipeline runs for errors. Each activity in the pipeline could have specific errors, like a copy activity failing due to an invalid source or destination. They should look at the error details in ADF, which might indicate authentication issues or missing data. The linked services for the source (storage) and sink (maybe another storage or database) need to have correct connection strings and credentials. If using parameterized datasets, ensure the parameters (like file paths or dates) are correctly passed.\n",
            "\n",
            "Networking could be another culprit. If the storage account is restricted to certain virtual networks or IPs, ADF's integration runtime needs to be allowed. If they're using a private endpoint, the DNS resolution must be correct. Also, checking service health for any outages in Azure services involved.\n",
            "\n",
            "Data format issues might cause pipelines to fail. If the cost export files are in CSV and the pipeline expects Parquet, that could cause errors. Validating the data structure and schema in the copy activity is important. Maybe the CSV has a header row that's not accounted for, or columns have changed.\n",
            "\n",
            "Long-term, setting up monitoring with alerts and using FinOps Hub's validation tools would help catch issues early. Implementing a DevOps approach for ADF pipelines with CI/CD can prevent deployment errors. Also, regular audits of the FinOps setup ensure everything stays aligned with best practices.\n",
            "\n",
            "Putting this all together, the step-by-step guide should start with verifying storage configuration, checking Cost Management exports, reviewing ADF pipeline errors, examining networking and permissions, validating data formats, and then implementing monitoring and governance practices. Each step should have actionable checks and solutions, referencing Microsoft docs where applicable.\n",
            "</think>\n",
            "\n",
            "Here's a structured approach to troubleshoot and resolve FinOps Hub data flow issues:\n",
            "\n",
            "### 1. Verify Storage Account Configuration\n",
            "**Immediate Checks:**\n",
            "```powershell\n",
            "# Check storage account network rules\n",
            "az storage account show --name <account-name> --query networkRuleSet\n",
            "```\n",
            "- Confirm storage account firewall allows Azure Services access\n",
            "- Verify ADF Managed Identity has **Storage Blob Data Contributor** role\n",
            "- Check container path matches `exports/resourceguid=*/data=*/snapshot=*` pattern\n",
            "\n",
            "**Common Fix:** Update networking rules and RBAC assignments in Storage Account > IAM\n",
            "\n",
            "### 2. Validate Cost Management Exports\n",
            "**Diagnosis Path:**\n",
            "1. Portal: Cost Management > Exports > Check \"Last export status\"\n",
            "2. Verify export scope matches intended resources\n",
            "3. Confirm export frequency aligns with pipeline triggers\n",
            "\n",
            "**Critical Check:** Ensure export files exist in storage account path:\n",
            "```bash\n",
            "az storage blob list --account-name <storage> --container-name <container> --prefix \"exports/\" --output table\n",
            "```\n",
            "\n",
            "### 3. Audit Data Factory Pipeline Components\n",
            "**Step-by-Step Pipeline Validation:**\n",
            "1. **Trigger Status:** Check if scheduled triggers are enabled/disabled\n",
            "2. **Linked Service Connections:**\n",
            "   - Storage account SAS token/Managed Identity authentication\n",
            "   - Validate connection with \"Test Connection\" button\n",
            "3. **Dataset Configuration:**\n",
            "   - Confirm file path parameters match export patterns\n",
            "   - Verify file format (CSV/Parquet) matches pipeline expectations\n",
            "4. **Copy Activity Monitoring:**\n",
            "   - Check input/output counts in pipeline run details\n",
            "   - Look for column mapping errors in failed runs\n",
            "\n",
            "**Troubleshooting Command:**\n",
            "```powershell\n",
            "# Get last 10 pipeline errors\n",
            "az datafactory pipeline-run query-by-factory --resource-group <rg> --factory-name <adf> --last-updated-after <date> --last-updated-before <date> --filters operand=\"Status\" operator=\"Equals\" values=\"Failed\"\n",
            "```\n",
            "\n",
            "### 4. Network & Security Validation\n",
            "**Key Checks:**\n",
            "- If using Private Endpoint: Validate DNS resolution for storage account\n",
            "- NSG/Firewall rules allowing AzureDataFactory IP ranges\n",
            "- Service Endpoints enabled for Storage in ADF subnet\n",
            "\n",
            "**Test Connectivity:**\n",
            "```powershell\n",
            "# From ADF Integration Runtime network\n",
            "Test-NetConnection <storage-account>.blob.core.windows.net -Port 443\n",
            "```\n",
            "\n",
            "### 5. Data Format Verification\n",
            "**Common CSV Issues:**\n",
            "- Header row mismatch between export schema and dataset definition\n",
            "- Date format inconsistencies (UTC vs local time)\n",
            "- Special characters in resource names breaking CSV parsing\n",
            "\n",
            "**Parquet Validation:**\n",
            "```python\n",
            "# Quick Python snippet to test parquet files\n",
            "import pandas as pd\n",
            "df = pd.read_parquet('https://<storage>.blob.core.windows.net/<path>')\n",
            "print(df.dtypes)\n",
            "```\n",
            "\n",
            "### 6. Implement Monitoring & Alerting\n",
            "**Long-Term Prevention:**\n",
            "```arm\n",
            "# ARM template snippet for pipeline failure alerts\n",
            "{\n",
            "  \"type\": \"microsoft.insights/activityLogAlerts\",\n",
            "  \"apiVersion\": \"2017-04-01\",\n",
            "  \"name\": \"ADF Pipeline Failure\",\n",
            "  \"location\": \"Global\",\n",
            "  \"properties\": {\n",
            "    \"condition\": {\n",
            "      \"allOf\": [\n",
            "        {\n",
            "          \"field\": \"category\",\n",
            "          \"equals\": \"DataFactory\"\n",
            "        },\n",
            "        {\n",
            "          \"field\": \"status\",\n",
            "          \"equals\": \"Failed\"\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  }\n",
            "}\n",
            "```\n",
            "\n",
            "### Recommended Validation Workflow:\n",
            "1. Manual test export from Cost Management\n",
            "2. Verify file appears in storage within 4 hours\n",
            "3. Run ADF pipeline manually with debug mode\n",
            "4. Check data transformation in intermediate stages\n",
            "5. Validate final dataset in Power BI using direct storage query\n",
            "\n",
            "### Critical Documentation References:\n",
            "- [Cost Management Export Troubleshooting](https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/troubleshoot-export-exports)\n",
            "- [ADF Copy Activity Error Codes](https://learn.microsoft.com/en-us/azure/data-factory/connector-common-errors)\n",
            "- [Storage Account Access Configuration](https://learn.microsoft.com/en-us/azure/storage/common/storage-network-security)\n",
            "\n",
            "**Final Recommendation:** Implement FinOps Health Check using [FinOps Hub Validation Toolkit](https://learn.microsoft.com/en-us/azure/cost-management-billing/finops/overview) to automatically detect configuration drift between components.\n",
            "\n",
            "By following this structured approach, you should be able to isolate whether the issue is in data generation (exports), data movement (ADF), or data consumption (Power BI) layers, and implement appropriate fixes at each stage.\n"
          ]
        }
      ],
      "source": [
        "finops_system_prompt = \"\"\"You are a Microsoft FinOps toolkit expert with deep knowledge of:\n",
        "\n",
        "1. FinOps Framework: The operational framework for financial management of cloud resources.\n",
        "2. Microsoft Cost Management: How to analyze, track, and allocate cloud spending.\n",
        "3. FinOps Hub Implementation: Setup, validation, and troubleshooting.\n",
        "4. Data Factory Pipelines: How they ingest and transform cost data.\n",
        "5. Power BI Cost Reports: Connection methods and optimization.\n",
        "6. VM Cost Optimization: Strategies for right-sizing, reservation, and scheduling VMs.\n",
        "\n",
        "When answering questions:\n",
        "- Provide clear, step-by-step guidance\n",
        "- Reference relevant Microsoft documentation\n",
        "- Include both immediate fixes and long-term strategies\n",
        "- Consider the business impact of cost optimization\n",
        "- Balance cost, performance, and security requirements\n",
        "\n",
        "Your goal is to help organizations maximize the business value derived from their cloud investment\n",
        "through the principles of FinOps: Visibility, Optimization, and Governance.\"\"\"\n",
        "\n",
        "def solve_finops_problem(problem):\n",
        "    \"\"\"Solve FinOps-related problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=f\"{problem} Please provide step-by-step analysis with actionable recommendations.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# FinOps optimization examples\n",
        "problem = \"\"\"Our organization recently deployed a FinOps hub but we're seeing errors in our Data Factory pipelines. \n",
        "How can we troubleshoot and ensure our Cost Management exports are properly flowing into our storage account for Power BI reporting?\"\"\"\n",
        "bi_problem = \"\"\"Our Power BI reports connected to our FinOps hub storage aren't showing the latest data. \n",
        "We're using a service principal to connect to storage. What should we verify to ensure proper data refresh?\"\"\"\n",
        "\n",
        "# Example 2: VM Cost Optimization\n",
        "vm_problem = \"\"\"Our Azure environment has 50 D-series and F-series VMs in West Europe running 24/7, \n",
        "but monitoring shows only 60% utilization during business hours and 15% during off-hours.\n",
        "How can we optimize costs while maintaining performance requirements?\"\"\"\n",
        "# Example 3: FinOps Hub Implementation\n",
        "hub_problem = \"\"\"We want to implement the Microsoft FinOps toolkit to get better visibility into our \n",
        "multi-subscription Azure environment. What's the best approach for a mid-sized organization \n",
        "with approximately $25K monthly Azure spend across 12 subscriptions?\"\"\"\n",
        "\n",
        "print(\"üîß FinOps Challenge:\", bi_problem)\n",
        "print(\"\\nüí° Solution:\", solve_finops_problem(problem))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59620846",
      "metadata": {},
      "outputs": [],
      "source": [
        "finops_system_prompt = \"\"\"You are a Microsoft FinOps toolkit expert with deep knowledge of:\n",
        "\n",
        "1. FinOps Framework: The operational framework for financial management of cloud resources.\n",
        "2. Microsoft Cost Management: How to analyze, track, and allocate cloud spending.\n",
        "3. FinOps Hub Implementation: Setup, validation, and troubleshooting.\n",
        "4. Data Factory Pipelines: How they ingest and transform cost data.\n",
        "5. Power BI Cost Reports: Connection methods and optimization.\n",
        "6. VM Cost Optimization: Strategies for right-sizing, reservation, and scheduling VMs.\n",
        "\n",
        "When answering questions:\n",
        "- Provide clear, step-by-step guidance\n",
        "- Reference relevant Microsoft documentation\n",
        "- Include both immediate fixes and long-term strategies\n",
        "- Consider the business impact of cost optimization\n",
        "- Balance cost, performance, and security requirements\n",
        "\n",
        "Your goal is to help organizations maximize the business value derived from their cloud investment\n",
        "through the principles of FinOps: Visibility, Optimization, and Governance.\"\"\"\n",
        "\n",
        "\n",
        "def analyze_vm_costs(vm_types, region, usage_pattern):\n",
        "    \"\"\"Analyze Azure VM costs with FinOps best practices\"\"\"\n",
        "    query = f\"\"\"As a FinOps specialist, analyze the cost-efficiency of {vm_types} in {region} with {usage_pattern} usage pattern.\n",
        "    Provide cost optimization recommendations including newer generation alternatives, reserved instances evaluation, \n",
        "    and scheduling opportunities. Include estimated monthly savings.\"\"\"\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=query)\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.5,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Example VM cost analysis\n",
        "vm_analysis = analyze_vm_costs(\n",
        "    \"Standard_D4s_v3 and Standard_F8s_v2\", \n",
        "    \"West Europe\", \n",
        "    \"60% during business hours, 15% during nights/weekends\"\n",
        ")\n",
        "print(\"üí∞ VM Cost Analysis:\", vm_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Problem: How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
            "Consider indexing strategies, hardware requirements, and query optimization.\n",
            "\n",
            "‚öôÔ∏è Solution: <think>\n",
            "Okay, so I need to figure out how to optimize a PostgreSQL database that's handling 10k transactions per second. That's a pretty high load, so I need to make sure everything is tuned properly. Let me start by breaking down the problem into parts. The user mentioned indexing strategies, hardware requirements, and query optimization. Maybe I should tackle each of these areas one by one.\n",
            "\n",
            "First, indexing. I know that indexes can speed up read queries, but they can also slow down writes because every insert, update, or delete has to update the index. So for a high transaction rate, I need to be careful not to over-index. But which indexes are essential? Probably primary keys are already indexed. Maybe look into composite indexes if there are common query patterns that filter on multiple columns. Also, using indexes on foreign keys if those are involved in joins. Wait, but maybe some indexes are redundant or not used. I should check for unused indexes and remove them to reduce overhead. Also, maybe using partial indexes if there are queries that filter on a specific value often. For example, if a status column has a few values and queries often look for 'active' records, a partial index on where status='active' could be smaller and faster. Also, considering the index type. B-tree is the default, but maybe BRIN for large tables with naturally ordered data like timestamps. BRIN is smaller and might be faster for range queries on such columns. Also, maybe covering indexes (INCLUDE) to include columns that are frequently accessed in queries, avoiding table lookups. But need to balance the index size here.\n",
            "\n",
            "Next, hardware requirements. 10k transactions per second is a lot. So the hardware needs to handle that. Let's think about CPU, RAM, storage, and network. For CPU, PostgreSQL is pretty CPU-intensive, especially for complex queries. So multiple cores would help, since PostgreSQL can parallelize some operations. Maybe a server with high-core count CPUs. For RAM, the more the better, right? The shared_buffers setting in PostgreSQL should be adjusted to use a portion of the RAM, typically around 25% of total RAM. But also, the OS cache is important. So having enough RAM to keep the working set in memory. If the database is, say, 100GB, then having 128GB of RAM would allow the entire database to be in memory. But if it's larger, maybe more RAM. For storage, SSDs are a must. High IOPS and low latency. Maybe NVMe drives. Also, considering the storage configuration: RAID 10 for redundancy and performance. Or if using cloud, provisioned IOPS volumes. Network-wise, the connection between the application and the database should be low-latency, maybe in the same data center. Also, sufficient bandwidth to handle the data transfer.\n",
            "\n",
            "Then, query optimization. Even with good hardware and indexes, bad queries can kill performance. So need to analyze slow queries using EXPLAIN ANALYZE or pg_stat_statements. Look for queries doing full table scans, missing indexes, or heavy joins. Maybe rewriting queries to be more efficient, using joins instead of subqueries, or avoiding SELECT *. Also, using prepared statements to reduce parsing overhead. Connection pooling is important here too. If the application is opening and closing connections for each transaction, that's a lot of overhead. Using a connection pooler like PgBouncer to reuse connections and manage them efficiently. Also, tuning PostgreSQL configuration parameters. Like increasing max_connections (but not too high, as each connection has overhead), work_mem for sort and hash operations, maintenance_work_mem for vacuum and index creation, effective_cache_size to let the planner know how much cache is available. Also, autovacuum settings. If there are a lot of writes, autovacuum needs to keep up to prevent bloat. Maybe increasing autovacuum_vacuum_cost_limit or lowering autovacuum_vacuum_cost_delay. Partitioning large tables. If a table is huge, partitioning by range or list can help with query performance and maintenance. For example, partitioning by time if dealing with time-series data. Sharding might be necessary if the database outgrows a single server, but that's more complex. Maybe using Citus or another sharding solution. Replication for read scalability. Using read replicas to offload read queries, allowing the primary to handle writes. But need to manage replication lag. Also, considering the use of connection pools to distribute read traffic to replicas.\n",
            "\n",
            "Wait, let me think about the configuration parameters again. shared_buffers is important, but the default is low. Maybe set to 8GB or more depending on total RAM. work_mem depends on the number of concurrent sorts and hashes. If it's too high, it can cause memory issues. Maybe 4MB to 16MB, but need to calculate based on expected concurrent operations. effective_cache_size should be set to the amount of memory available for disk caching, which is the total RAM minus shared_buffers and other system usages. autovacuum should be tuned to run more aggressively. Maybe setting autovacuum_max_workers higher if there are many tables. Also, checkpoints: increasing checkpoint_timeout and checkpoint_completion_target to spread out write I/O. But need to balance with recovery time.\n",
            "\n",
            "Another thing: transaction isolation levels. Using the appropriate level to minimize locking. For example, using READ COMMITTED instead of SERIALIZABLE if possible. Also, avoiding long-running transactions that hold locks and cause vacuuming issues.\n",
            "\n",
            "What about connection pooling? If the application is using a lot of connections, each with their own overhead, PgBouncer in transaction pooling mode can help. But prepared statements might not work well with transaction pooling. So need to see if the application uses prepared statements. If so, maybe use session pooling instead, but that requires more connections. Alternatively, configure the application to use prepared statements sparingly.\n",
            "\n",
            "Monitoring is also key. Using tools like pg_stat_activity to see current queries, pg_stat_statements to find slow queries, and monitoring disk I/O, CPU, memory usage. Maybe setting up alerts for long-running transactions or lock contention.\n",
            "\n",
            "Hardware redundancy: having a standby replica for failover. Using streaming replication with synchronous or asynchronous replication. If synchronous, it can impact performance but ensures no data loss. Asynchronous is faster but riskier.\n",
            "\n",
            "Also, considering the use of UNLOGGED tables for temporary data that can be recreated, as they are faster but not crash-safe. Or increasing the WAL settings, like wal_buffers and checkpoint settings.\n",
            "\n",
            "Wait, what about write-ahead log (WAL) settings? Tuning wal_level, wal_buffers, and max_wal_size. For high write throughput, maybe increasing max_wal_size to allow larger WAL segments before checkpointing. Also, ensuring that the WAL is on a fast disk separate from the main data storage if possible.\n",
            "\n",
            "Another point: using the latest PostgreSQL version, as performance improvements are made in each release. Maybe upgrading to the latest stable version if not already there.\n",
            "\n",
            "Materialized views could help for complex queries that are run frequently but don't need real-time data. Refreshing them periodically to offload the processing from the main transactions.\n",
            "\n",
            "Lock contention: using row-level locks where possible, avoiding table locks. Also, keeping transactions short to minimize lock duration.\n",
            "\n",
            "In summary, the main areas are:\n",
            "\n",
            "1. Indexing: Proper indexes, remove unused, use partial/BRIN, covering indexes.\n",
            "2. Hardware: Fast CPUs, ample RAM, SSDs, good network.\n",
            "3. Query optimization: Analyze and tune queries, use EXPLAIN, avoid bottlenecks.\n",
            "4. Configuration: Tune PostgreSQL settings (shared_buffers, work_mem, autovacuum, etc.).\n",
            "5. Connection pooling: Use PgBouncer to manage connections.\n",
            "6. Partitioning and replication: Scale horizontally where needed.\n",
            "7. Monitoring and maintenance: Regular vacuuming, analyze, and monitoring.\n",
            "\n",
            "I need to make sure all these points are covered in the answer. Maybe prioritize the most impactful ones first. For example, proper indexing and query optimization can have immediate effects. Then hardware, configuration, connection pooling, etc.\n",
            "</think>\n",
            "\n",
            "To optimize a PostgreSQL database handling **10k transactions/second**, implement a holistic approach combining indexing, hardware tuning, query optimization, and configuration adjustments:\n",
            "\n",
            "**1. Indexing Strategies:**\n",
            "- **Targeted Indexes:** Use composite indexes for common query patterns and partial indexes for frequent filters (e.g., `WHERE status = 'active'`). Avoid over-indexing to reduce write overhead.\n",
            "- **Index Types:** Use BRIN for large, ordered datasets (e.g., timestamps) and covering indexes (`INCLUDE`) to avoid table lookups.\n",
            "- **Maintenance:** Remove unused/redundant indexes and reindex regularly to prevent bloat.\n",
            "\n",
            "**2. Hardware Requirements:**\n",
            "- **CPU:** High-core count processors to parallelize operations.\n",
            "- **RAM:** ‚â•128GB to cache working datasets (adjust `shared_buffers` to ~25% of RAM).\n",
            "- **Storage:** NVMe SSDs with RAID 10 or cloud-provisioned IOPS for low-latency I/O.\n",
            "- **Network:** Low-latency, high-bandwidth connections between app and DB.\n",
            "\n",
            "**3. Query Optimization:**\n",
            "- **Analyze Queries:** Use `EXPLAIN ANALYZE` and `pg_stat_statements` to identify slow queries. Avoid full scans, optimize joins, and minimize `SELECT *`.\n",
            "- **Prepared Statements:** Reduce parsing overhead with parameterized queries.\n",
            "- **Connection Pooling:** Use PgBouncer in transaction/session mode to manage connection churn.\n",
            "\n",
            "**4. PostgreSQL Configuration:**\n",
            "- **Memory Settings:** Increase `work_mem` (4‚Äì16MB) for sorts/hashes and `effective_cache\n"
          ]
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            UserMessage(content=f\"{problem} Please reason step by step, and put your final answer within \\boxed{{}}.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
        "Consider indexing strategies, hardware requirements, and query optimization.\"\"\"\n",
        "\n",
        "print(\"üîß Problem:\", problem)\n",
        "print(\"\\n‚öôÔ∏è Solution:\", solve_technical_problem(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "054af028",
      "metadata": {},
      "source": [
        "## FINOPS TOOLKIT TECH SUPPORT BOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e0a07253",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Client initialized | Model: deepseek-r1\n",
            "üîß FinOps Challenge: Our organization recently deployed a FinOps hub but we're seeing errors in our Data Factory pipelines. \n",
            "How can we troubleshoot and ensure our Cost Management exports are properly flowing into our storage account for Power BI reporting?\n",
            "\n",
            "üí° Solution: <think>\n",
            "Okay, let's tackle this problem. The user has deployed a FinOps hub and is encountering errors in their Data Factory pipelines. They want to ensure that Cost Management exports are correctly flowing into their storage account for Power BI reporting. \n",
            "\n",
            "First, I need to recall the troubleshooting steps for Data Factory pipelines in the context of FinOps Hub. The user mentioned errors, so my initial thought is to check the pipeline runs for any failures. The steps from the previous knowledge mention verifying Cost Management exports, Data Factory pipelines, storage containers, and Power BI configuration.\n",
            "\n",
            "Starting with Cost Management exports, I should verify if the exports are actually succeeding. If the exports aren't generating the initial data, then the pipelines won't have anything to process. The user should check the storage account's msexports container for the raw CSV files. If those are missing, the problem is at the export level, maybe permissions or configuration issues.\n",
            "\n",
            "Next, Data Factory pipelines. The user needs to check the pipeline runs in ADF. Look for any failed runs and examine the error messages. Common issues could be triggers not firing, like the msexports_ManifestAdded trigger not starting. Also, check if the necessary resource providers are registered, such as Microsoft.DataFactory and Microsoft.Storage. Sometimes, the pipelines might fail due to column mapping errors in the ETL process, especially if there are changes in the CSV schema from Cost Management exports.\n",
            "\n",
            "Then, the storage containers: after the pipelines run, the processed data should be in the ingestion container as Parquet files. If the raw files are present but Parquet files are missing, the issue is within the Data Factory pipelines. Maybe the pipelines aren't triggered, or there's a transformation error.\n",
            "\n",
            "Power BI configuration comes next. If the data is in the storage account, but reports aren't updating, the problem could be with the dataset connection. Check the data source credentials in Power BI and ensure it's pointing to the correct storage location.\n",
            "\n",
            "Also, considering the FinOps principles, after fixing the immediate issues, the user should set up monitoring and alerts for future pipeline failures. Maybe implement logging and use Azure Monitor to track pipeline health. Long-term, regular validation of the entire data flow can prevent such issues from recurring.\n",
            "\n",
            "I should structure the answer step-by-step, starting with verifying the Cost Management exports, then Data Factory pipelines, checking storage containers, and Power BI. Include both immediate actions and long-term strategies. Make sure to mention specific error checks like triggers, resource providers, and schema changes. Also, emphasize the business impact of having accurate cost reporting for optimization and governance.\n",
            "</think>\n",
            "\n",
            "Here's a structured approach to diagnose and resolve your FinOps Hub data flow issues:\n",
            "\n",
            "**1. Verify Cost Management Exports (Source Validation)**\n",
            "- *Check Export Status*: \n",
            "  - Go to Azure Portal > Cost Management > Exports\n",
            "  - Confirm last successful export matches your schedule (green checkmark)\n",
            "  - Check for \"Last export status\" errors\n",
            "- *Validate Raw Data*:\n",
            "  - Navigate to Storage Account > `msexports` container\n",
            "  - Verify CSV files exist with pattern `Consumption_{date}.csv`\n",
            "  - Minimum file size check: Recent files should be >1KB\n",
            "\n",
            "**2. Diagnose Data Factory Pipelines (Transformation Layer)**\n",
            "- *Check Pipeline Triggers*:\n",
            "  ```powershell\n",
            "  # Verify Data Factory resource provider registration\n",
            "  Get-AzResourceProvider -ProviderNamespace Microsoft.DataFactory\n",
            "  ```\n",
            "  - In ADF Studio > Monitor > Trigger Runs\n",
            "  - Confirm `msexports_ManifestAdded` trigger is firing\n",
            "- *Analyze Pipeline Errors*:\n",
            "  - Open failed pipeline runs > \"Activity Runs\" tab\n",
            "  - Look for specific error codes like:\n",
            "    - `AuthorizationFailed`: Storage permissions issue\n",
            "    - `ColumnNotFound`: Schema mismatch in CSV\n",
            "    - `Timeout`: Increase pipeline TTL in parameters\n",
            "- *Validate Data Transformation*:\n",
            "  - Check `fincm_costmanagement_export` pipeline\n",
            "  - Confirm mapping contains all required columns including new Azure columns\n",
            "  - Test sample CSV through mapping manually\n",
            "\n",
            "**3. Validate Storage Output (Sink Validation)**\n",
            "- *Check Processed Data*:\n",
            "  - Navigate to Storage Account > `ingestion` container\n",
            "  - Verify partitioned Parquet files exist in:\n",
            "    `processed/fincm/costmanagement/year={year}/month={month}/day={day}/`\n",
            "  - Use Storage Explorer to preview Parquet content\n",
            "- *Check Error Logs*:\n",
            "  - Review `error` container for failed records\n",
            "  - Check `processed/fincm/costmanagement/_errorlogs`\n",
            "\n",
            "**4. Power BI Connectivity (Consumption Layer)**\n",
            "- *Dataset Validation*:\n",
            "  ```powerquery\n",
            "  // Sample Power Query connection test\n",
            "  let\n",
            "    Source = AzureStorage.Blobs(\"https://yourstorage.blob.core.windows.net/ingestion\"),\n",
            "    ingestion = Source{[Name=\"ingestion\"]}[Data]\n",
            "  in\n",
            "    ingestion\n",
            "  ```\n",
            "  - Check data gateway status if using on-premises gateway\n",
            "  - Validate storage account RBAC: Reader role for Power BI service principal\n",
            "\n",
            "**Immediate Actions:**\n",
            "1. For pipeline authorization errors:\n",
            "   - Revalidate Managed Identity permissions:\n",
            "     ```bash\n",
            "     az role assignment list --assignee <data-factory-MI> --scope /subscriptions/<sub-id>\n",
            "     ```\n",
            "     Ensure Storage Blob Data Contributor role at storage account level\n",
            "\n",
            "2. For schema mismatch errors:\n",
            "   - Update mapping in `TypeConversion` pipeline activity\n",
            "   - Add new columns to `type-conversion.json` schema definition\n",
            "\n",
            "**Long-Term Prevention:**\n",
            "```azurecli\n",
            "# Create monitoring alert for pipeline failures\n",
            "az monitor scheduled-query create \\\n",
            "  --name \"ADF-Pipeline-Failure-Alert\" \\\n",
            "  --resource-group <rg-name> \\\n",
            "  --scopes /subscriptions/<sub-id>/resourceGroups/<rg>/providers/Microsoft.DataFactory/factories/<adf-name> \\\n",
            "  --condition \"count \\'ADFActivityRun\\' | where Result == \\'Failed\\' > 0\" \\\n",
            "  --description \"Alert on Data Factory pipeline failures\"\n",
            "```\n",
            "\n",
            "**Business Impact Mitigation:**\n",
            "While troubleshooting:\n",
            "1. Use direct Cost Management API calls for critical reports:\n",
            "   ```python\n",
            "   # Temporary Python script to fetch cost data\n",
            "   from azure.costmanagement import CostManagementClient\n",
            "   client = CostManagementClient(credential)\n",
            "   query = {\"type\": \"ActualCost\", \"dataset\": {\"granularity\": \"Daily\"}}\n",
            "   result = client.query.usage(\"<scope>\", query)\n",
            "   ```\n",
            "2. Enable fallback to raw CSV exports in Power BI with direct storage connection\n",
            "\n",
            "**Final Validation Checklist:**\n",
            "1. End-to-end flow test:\n",
            "   - Trigger manual export in Cost Management\n",
            "   - Verify file appears in `msexports` within 15 mins\n",
            "   - Confirm pipeline processes file to `ingestion` within 30 mins\n",
            "   - Refresh Power BI dataset to see new data\n",
            "\n",
            "2. Performance optimization:\n",
            "   - Check pipeline duration in ADF Monitor (<2 mins per file normal)\n",
            "   - If slow processing, increase Data Flow cluster core count\n",
            "\n",
            "By following this structured approach, you'll systematically identify where the breakdown is occurring in the FinOps data pipeline while maintaining business continuity through temporary workarounds.\n",
            "\n",
            "üí∞ VM Cost Analysis: <think>\n",
            "Okay, let's tackle this user's question about analyzing the cost-efficiency of two Azure VM types: Standard_D4s_v3 and Standard_F8s_v2 in West Europe. The usage pattern is 60% during business hours and 15% during nights/weekends. They want recommendations on newer generations, reserved instances, scheduling, and estimated savings. \n",
            "\n",
            "First, I need to recall the specs of these VMs. The D4s_v3 is part of the Dv3 series, which are general-purpose VMs. It has 4 vCPUs, 16 GB RAM. The F8s_v2 is from the Fsv2 series, which are compute-optimized. It has 8 vCPUs and 16 GB RAM. The Fsv2 uses 2nd Gen Intel Xeon Scalable processors, so they might offer better performance per core compared to Dv3. \n",
            "\n",
            "The user's workload runs 60% during business hours (assume 12 hours/day, 22 days/month) and 15% during nights/weekends (remaining time). So right off the bat, scheduling shutdowns during off-hours could save costs. But they might already be considering that, so I need to check both scenarios: if they're currently running 24/7 vs. already scaling down. Wait, the problem states the usage pattern is 60% during business hours and 15% during nights/weekends. That suggests they're already scaling down, but maybe not optimally. Or perhaps the 60% and 15% are utilization rates, not uptime? Wait, the phrasing says \"usage pattern\" with percentages. Hmm, need to clarify. But the user might mean uptime. Let me confirm the standard approach here. Typically, when users mention usage patterns with percentages, it's the uptime, like running 60% of the time during business hours. Or maybe the CPU utilization? The question isn't entirely clear. But given the context of scheduling opportunities, it's more likely about uptime. So maybe the VMs are running 24/7 but only utilized 60% during business hours and 15% otherwise. Wait, no‚Äîif they're looking for scheduling, perhaps the VMs can be stopped during off-hours. So perhaps the current setup is running 24/7, but the actual usage is 60% during business hours and 15% at other times. But for cost optimization, turning them off when not needed would save more. Alternatively, maybe the user is already scaling the VMs up and down, but the usage percentages refer to utilization, not uptime. But the user mentions \"scheduling opportunities,\" which implies that turning off VMs during low usage times is a possibility. So I need to consider both scenarios: current state (assuming 24/7 running) and potential savings by shutting down during low usage periods.\n",
            "\n",
            "Wait, the user's exact wording is: \"60% during business hours, 15% during nights/weekends usage pattern.\" This might mean that the VM is used 60% of the time during business hours (maybe 12 hours a day) and 15% during nights and weekends. But that's a bit ambiguous. Alternatively, it could be the utilization percentage. But given that the user is asking about scheduling opportunities, it's more likely that the VM is needed 60% of business hours (like 60% uptime during the day) and 15% during nights. But this is unclear. To proceed, I should make an assumption and state it. Let me assume that the VM runs 60% of the time during business hours (which are say 12 hours a day, 22 days a month) and 15% during nights and weekends (the remaining 12 hours a day on weekdays and 24 on weekends). But that might not add up. Alternatively, perhaps the VM is required to be available 60% of the time during business hours (e.g., scaled up) and 15% during off-hours. This is confusing. Alternatively, maybe the VM's CPU utilization is 60% during business hours and 15% otherwise, but that's about performance, not cost. Since the user is asking about scheduling, I think the correct interpretation is that the VM is needed to run 60% of the business hours (maybe 12 hours a day) and 15% of nights/weekends. Wait, maybe the usage is 60% of the day (14.4 hours) during business days and 15% on nights and weekends. But this is getting too tangled. The user probably means that the VM is utilized (needs to run) 60% of the time during business hours (which are, say, 8am-8pm, 12 hours, 22 days a month) and 15% during nights and weekends (the remaining 12 hours a day and weekends). So total uptime would be (12 hours * 22 days * 60%) + (12 hours * 22 days + 48 hours weekends) * 15%. Let's calculate that. But maybe the user just wants to consider that the VM is running 24/7, but the workload's utilization is 60% and 15%, so right-sizing could be possible. Alternatively, maybe the VM can be turned off during non-business hours. But given the way the question is phrased, I think the user is looking for scheduling opportunities where the VM can be shut down during nights and weekends, thus reducing the uptime from 24/7 to just business hours. The 60% and 15% might refer to CPU utilization, indicating potential for right-sizing. Hmm. This is a critical point. If the VM's CPU utilization is 60% during business hours and 15% otherwise, that suggests that the VM is over-provisioned, and a smaller size could be used. Alternatively, if it's about uptime, then turning off the VM when not needed. The user mentions \"scheduling opportunities,\" so I need to address both possibilities: right-sizing and scheduling. \n",
            "\n",
            "So, step by step:\n",
            "\n",
            "1. Analyze current costs of D4s_v3 and F8s_v2 in West Europe, considering their hourly rates.\n",
            "\n",
            "2. Determine if the workload can be scheduled to turn off during nights and weekends, reducing compute hours.\n",
            "\n",
            "3. Evaluate newer VM generations that offer better price-performance.\n",
            "\n",
            "4. Consider Reserved Instances for the baseline usage (the portion that runs continuously).\n",
            "\n",
            "5. Calculate potential savings from scheduling, right-sizing, reserved instances, and newer generations.\n",
            "\n",
            "First, get the current pay-as-you-go prices for these VMs in West Europe. Let me check the current prices (as of 2023-10, since real-time data isn't available). \n",
            "\n",
            "Standard_D4s_v3: 4 vCPUs, 16 GB RAM. West Europe price is approximately $0.192/hour (Linux, pay-as-you-go).\n",
            "\n",
            "Standard_F8s_v2: 8 vCPUs, 16 GB RAM. Price around $0.299/hour (Linux, PAYG).\n",
            "\n",
            "Assuming both are running 24/7 for a month (730 hours), the monthly cost would be:\n",
            "\n",
            "D4s_v3: 730 * 0.192 = ~$140.16\n",
            "\n",
            "F8s_v2: 730 * 0.299 = ~$218.27\n",
            "\n",
            "But the usage pattern is 60% during business hours and 15% during nights/weekends. Wait, the user's exact phrasing is \"60% during business hours, 15% during nights/weekends usage pattern.\" This is ambiguous. It could mean:\n",
            "\n",
            "- The VM is actively used (CPU utilization) at 60% during business hours and 15% during off-hours. This would suggest that the VM is running 24/7 but underutilized, so right-sizing is possible.\n",
            "\n",
            "- The VM is required to be running only 60% of the time during business hours and 15% during off-hours, which would imply scheduling opportunities.\n",
            "\n",
            "But the user mentions \"scheduling opportunities\" as part of the recommendations, so perhaps the VM can be stopped during certain periods. Let me assume that the VM is needed 60% of business hours (e.g., 12 hours a day * 60% = 7.2 hours per weekday) and 15% during nights/weekends (12 hours * 15% = 1.8 hours on weeknights and similar on weekends). Alternatively, perhaps the VM is running 24/7, but the actual utilization is 60% and 15%, so right-sizing to a smaller VM could save costs. Let me clarify both scenarios.\n",
            "\n",
            "First, if the VM is running 24/7 but underutilized, right-sizing is the key. For example, if the D4s_v3 is at 60% CPU during business hours and 15% otherwise, maybe a smaller VM like D2s_v3 could handle the workload, leading to cost savings.\n",
            "\n",
            "Alternatively, if the VM can be stopped during off-hours, then compute hours can be reduced. Let's calculate the potential savings from scheduling.\n",
            "\n",
            "Assuming business hours are 12 hours/day on weekdays (22 days/month), and off-hours are 12 hours/weekday nights + 48 hours weekends = 12*22 + 48 = 264 + 48 = 312 hours/month.\n",
            "\n",
            "If the VM is running 60% during business hours (0.6 * 12 * 22 = 158.4 hours) and 15% during off-hours (0.15 * 312 = 46.8 hours), total hours/month = 205.2. \n",
            "\n",
            "But this seems low. Alternatively, maybe the user means that during business hours, the VM is used 60% of the time (i.e., 60% uptime), and during nights/weekends, 15% uptime. So total compute hours\n",
            "\n",
            "‚ùì Question: How do I validate my FinOps hub deployment?\n",
            "\n",
            "üß† Thinking Process: Okay, the user is asking how to validate their FinOps hub deployment. Let me recall the steps from the documentation they provided. The main sections are verifying Cost Management exports and Data Factory pipelines, plus checking storage containers and Power BI.\n",
            "\n",
            "First, I need to outline the validation process step by step. The user might be encountering issues, so being thorough is important. Starting with Cost Management exports: check if the status is successful. If not, ensure the resource provider is registered. Maybe mention filing a support request if it's still not working.\n",
            "\n",
            "Next, Data Factory pipelines. They need to check the Monitor section in Data Factory Studio for successful runs. If there are failures, look at error messages and check the trigger. The msexports_ManifestAdded trigger should be started. Also, comparing the run time with the export time can indicate if there's a delay or sync issue.\n",
            "\n",
            "Then, storage containers. The msexports container should have raw CSV files, and the ingestion container should have Parquet files. If those are missing, the pipeline might not be processing data correctly. Maybe suggest checking the pipeline's source and sink configurations.\n",
            "\n",
            "Power BI configuration comes next. They need to ensure the dataset is connected to the Synapse workspace and that data is refreshing. If there are errors, checking the data source permissions and query parameters would help.\n",
            "\n",
            "Troubleshooting tips: common issues like triggers not starting, resource providers not registered, or column mapping errors in ETL. Suggest checking each of these areas. Also, mention long-term strategies like monitoring regularly and setting up alerts.\n",
            "\n",
            "I should structure the answer clearly, using the steps from the documentation but expanding with practical advice. Make sure to cover each validation point and possible fixes. The user might not know where to look for these components, so being specific about navigation paths (like where to find triggers in Data Factory) is helpful.\n",
            "\n",
            "Also, considering the business impact, emphasize how proper validation ensures accurate cost visibility and optimization, aligning with FinOps principles. Maybe add a note on governance by suggesting regular audits.\n",
            "\n",
            "Wait, the user's documentation also mentioned VM cost optimization strategies. Should I include that here? The query is about deployment validation, so maybe not directly. Focus on the validation steps first. But if the user is setting up FinOps hub, ensuring that VM optimization is part of their strategy could be a long-term tip. Perhaps mention that after validation, they can proceed to optimize VMs using the data from the hub.\n",
            "\n",
            "Putting it all together: start with the validation steps in order, then troubleshooting common issues, and finally best practices. Use bullet points or numbered lists for clarity. Make sure each step is actionable and references the specific parts of the FinOps toolkit documentation they provided.\n",
            "\n",
            "üìù Final Answer: Here's a structured validation process for your FinOps hub deployment, following Microsoft's recommended best practices:\n",
            "\n",
            "### Step-by-Step Validation Guide\n",
            "\n",
            "**1. Cost Management Export Verification**\n",
            "```powershell\n",
            "# Check export status via Azure Portal\n",
            "Go to Cost Management > Exports > Verify:\n",
            "- Last export status shows \"Succeeded\"\n",
            "- Export frequency matches your configuration (daily/weekly)\n",
            "- Data matches expected time range (UTC timestamps)\n",
            "```\n",
            "- **If exports fail**:  \n",
            "  `Register-AzResourceProvider -ProviderNamespace Microsoft.CostManagement`  \n",
            "  Check RBAC permissions (Requires Cost Management Contributor role)\n",
            "\n",
            "**2. Data Factory Pipeline Health Check**\n",
            "```powershell\n",
            "# In Data Factory Studio:\n",
            "Monitor > Pipeline Runs (Last 24 hours):\n",
            "- All pipeline runs should show Succeeded status\n",
            "- msexports_ManifestAdded trigger shows \"Started\" state\n",
            "- Execution timestamps align with Cost Export completion\n",
            "```\n",
            "- **Critical Metrics to Validate**:\n",
            "  - `ProcessedFileCount` > 0 in pipeline output\n",
            "  - `ConsumptionStart`/`ConsumptionEnd` dates cover full period\n",
            "  - Zero files in `errorcontainer`\n",
            "\n",
            "**3. Storage Validation (Hierarchical Check)**\n",
            "```markdown\n",
            "1. **Raw Data Layer** (msexports container):\n",
            "   - Verify CSV files exist in `inbox/ms-exports/` path\n",
            "   - Check timestamps match export schedule\n",
            "\n",
            "2. **Processed Data Layer** (ingestion container):\n",
            "   - Confirm Parquet files in `processed/` directory\n",
            "   - Validate file sizes (typically 50-200MB per file)\n",
            "   - Check for _SUCCESS flag files\n",
            "```\n",
            "\n",
            "**4. Power BI Integration Audit**\n",
            "```powershell\n",
            "# In Power BI Service:\n",
            "Check Dataset Settings > Data Source Credentials:\n",
            "- Azure Synapse connection shows \"Valid\"\n",
            "- Scheduled refresh history shows recent successes\n",
            "- Verify report filters match your:  \n",
            "   - Subscription IDs\n",
            "   - Resource groups\n",
            "   - Cost centers\n",
            "```\n",
            "\n",
            "### Common Troubleshooting Scenarios\n",
            "\n",
            "**If Data Factory Pipelines Fail:**\n",
            "1. Check for `MappingColumnNotFound` errors in Copy Data activities\n",
            "2. Validate ARM template deployment completed successfully\n",
            "3. Ensure storage account firewall allows Data Factory IPs\n",
            "4. Confirm Managed Identity has **Storage Blob Data Contributor** role\n",
            "\n",
            "**For Persistent Issues:**\n",
            "```markdown\n",
            "- **Error Code**           | **Immediate Action**\n",
            "- ADFRuntimeError         | Check network security groups\n",
            "- AuthorizationFailed     | Revalidate RBAC assignments\n",
            "- ResourceNotFound        | Verify linked service configurations\n",
            "```\n",
            "\n",
            "### Pro Tips for Long-Term Success\n",
            "1. **Governance**  \n",
            "   Set Azure Monitor alerts for:\n",
            "   - Failed pipeline runs\n",
            "   - Export completion delays >2 hours\n",
            "   - Storage account write operations <10/day\n",
            "\n",
            "2. **Optimization**  \n",
            "   Use validation data to:\n",
            "   - Establish cost anomaly detection thresholds\n",
            "   - Calculate VM right-sizing metrics (CPU% <40% && Memory% <60%)\n",
            "   - Identify reservation purchase opportunities\n",
            "\n",
            "3. **Visibility**  \n",
            "   Schedule weekly validation checks:\n",
            "   - Storage account growth rate\n",
            "   - Pipeline execution duration trends\n",
            "   - Power BI dataset refresh performance\n",
            "\n",
            "This validation process ensures your FinOps hub maintains the CIA triad of cloud financial management: **C**ontinuous data flow, **I**ntegrity of processed information, and **A**ctionable cost visibility. Would you like me to elaborate on any specific component?\n",
            "\n",
            "üîç Power BI Issue: Our Power BI reports connected to our FinOps hub storage aren't showing the latest data. \n",
            "We're using a service principal to connect to storage. What should we verify to ensure proper data refresh?\n",
            "\n",
            "üîß Troubleshooting Steps: <think>\n",
            "Okay, let's tackle this user's problem. Their Power BI reports connected to the FinOps hub aren't showing the latest data, and they're using a service principal. I need to walk through the validation steps as per the FinOps toolkit.\n",
            "\n",
            "First, I'll start with the known error codes. Maybe there's a specific error related to service principal permissions or data refresh failures. They should check Power BI's dataset refresh history for any errors. If there's an error code, cross-referencing it with Microsoft's documentation could give a quick fix.\n",
            "\n",
            "Next, validating each component in sequence. Starting with Cost Management exports. If the exports aren't running, there's no new data. They need to ensure the exports are active and the storage account is correctly linked. Checking the msexports container for recent CSV files would confirm if data is being exported properly.\n",
            "\n",
            "Then, Data Factory pipelines. The pipelines process the raw data into parquet files. They should check if the pipelines are running on schedule, especially the msexports_ManifestAdded trigger. Any failed pipeline runs would indicate issues like missing columns or permission problems. Also, verifying that the ingestion container has the latest parquet files is crucial.\n",
            "\n",
            "Storage account containers: msexports should have new CSVs, and ingestion should have parquet files. If the parquet files aren't updated, the Data Factory pipeline might be the culprit. They need to check the folder structure in ingestion to ensure it's organized by year/month/day.\n",
            "\n",
            "Now, focusing on Power BI configuration. Since they're using a service principal, permissions are key. The service principal needs Storage Blob Data Reader role on both containers. They should verify the role assignments in Azure. Also, the Power BI dataset connection string must use the correct storage account path and the service principal's credentials. Maybe the credentials expired or the secret was updated without refreshing in Power BI.\n",
            "\n",
            "Data model relationships in Power BI could be another issue. If the schema changed, like new columns added, the Power BI model might not handle it, causing missing data. They should check for schema changes and update the model accordingly.\n",
            "\n",
            "Scheduled refresh settings in Power BI need to be checked. The refresh might be set to a frequency that's too slow, or there might be gateway issues if on-prem data is involved. They should ensure the refresh schedule aligns with the data update frequency from Azure.\n",
            "\n",
            "Lastly, incremental refresh policies in Power BI could be filtering out new data if not configured properly. They need to review the policy to make sure it's capturing the latest dates.\n",
            "\n",
            "Putting it all together, the user should go through each layer step by step, starting from data export, through processing, to Power BI configuration. Ensuring each step is functioning and permissions are correctly set, especially for the service principal, should resolve the data refresh issue.\n",
            "</think>\n",
            "\n",
            "Here's a detailed validation sequence for your Power BI data refresh issue, following Microsoft FinOps toolkit best practices:\n",
            "\n",
            "**1. Verify Cost Management Exports**\n",
            "```powershell\n",
            "# Check last export execution (Azure Portal > Cost Management > Exports)\n",
            "Get-AzConsumptionExport -ExportName \"YourExportName\" | Select-Object Status, LastExecutionTime\n",
            "```\n",
            "- Confirm exports show \"Succeeded\" status\n",
            "- Validate new CSV files exist in `msexports` container with current timestamps\n",
            "- Minimum required files: `NewBillingExportFiles.csv` and daily cost files\n",
            "\n",
            "**2. Validate Data Factory Pipelines**\n",
            "```powershell\n",
            "# Check pipeline runs (Azure Portal > Data Factory > Monitor)\n",
            "Get-AzDataFactoryV2PipelineRun -ResourceGroupName \"YourRG\" -DataFactoryName \"YourDF\" `\n",
            "    -LastUpdatedAfter (Get-Date).AddHours(-24) -LastUpdatedBefore (Get-Date)\n",
            "```\n",
            "- Confirm \"FinOpsETL\" pipeline has successful runs since last cost export\n",
            "- Check for \"msexports_ManifestAdded\" trigger activation\n",
            "- Verify error logs for common issues:\n",
            "  - `ErrorCode=ParquetInvalidColumn` (schema mismatch)\n",
            "  - `AuthorizationFailed` (service principal permissions)\n",
            "\n",
            "**3. Storage Account Validation**\n",
            "```markdown\n",
            "| Container     | Expected Content                          | Permission Requirements          |\n",
            "|---------------|-------------------------------------------|-----------------------------------|\n",
            "| msexports     | Raw CSV files with timestamps <2 hours old | Service Principal: Storage Reader |\n",
            "| ingestion     | Parquet files in partitioned folders      | Service Principal: Storage Blob   |\n",
            "|               | (e.g., year=2023/month=07/day=25)         | Data Contributor                 |\n",
            "```\n",
            "- Confirm parquet files exist in `ingestion` container with current dates\n",
            "- Verify folder structure matches Power BI parameters\n",
            "\n",
            "**4. Power BI Service Principal Configuration**\n",
            "```powershell\n",
            "# Check Azure AD app permissions\n",
            "Get-AzRoleAssignment -ServicePrincipalName \"YourSPN-AppID\" `\n",
            "    -Scope \"/subscriptions/{subId}/resourceGroups/{rg}/providers/Microsoft.Storage/storageAccounts/{storageName}\"\n",
            "```\n",
            "- Required roles:\n",
            "  - **Storage Blob Data Reader** on both containers\n",
            "  - **Reader** on storage account\n",
            "- Validate Power BI credentials:\n",
            "  1. In Power BI Service > Dataset Settings\n",
            "  2. Check authentication method is \"OAuth2\"\n",
            "  3. Confirm Service Principal ID matches Azure AD App ID\n",
            "  4. Verify secret hasn't expired\n",
            "\n",
            "**5. Power BI Data Model Validation**\n",
            "- Check query folding in Power Query:\n",
            "  ```powerquery-m\n",
            "  let\n",
            "    Source = AzureStorage.DataLake(\"https://{storage}.blob.core.windows.net/ingestion\"),\n",
            "    #\"Filtered Rows\" = Table.SelectRows(Source, each [Date] >= #datetime(2023,7,1,0,0,0))\n",
            "  in\n",
            "    #\"Filtered Rows\"\n",
            "  ```\n",
            "- Verify:\n",
            "  - Partition filtering is applied\n",
            "  - Schema matches latest parquet format\n",
            "  - All required fields are visible in \"Fields\" pane\n",
            "\n",
            "**6. Refresh Configuration**\n",
            "- In Power BI Service:\n",
            "  1. Check scheduled refresh frequency (minimum 8x/day for FinOps)\n",
            "  2. Verify no gateway is required (direct cloud-to-cloud connection)\n",
            "  3. Confirm no query size limits are truncating data\n",
            "  4. Validate incremental refresh policy (if used):\n",
            "     ```json\n",
            "     {\n",
            "       \"refreshPolicy\": {\n",
            "         \"type\": \"incremental\",\n",
            "         \"incrementalPeriods\": 1,\n",
            "         \"currentPeriod\": \"CustomQuery\"\n",
            "       }\n",
            "     }\n",
            "     ```\n",
            "\n",
            "**Common Fixes:**\n",
            "1. If seeing error `DM_GWPipeline_Gateway_DataSourceAccessError`:\n",
            "   - Re-authenticate service principal credentials\n",
            "   - Update storage account firewall rules to allow Power BI IP ranges\n",
            "\n",
            "2. For schema mismatch warnings:\n",
            "   - Run manual full processing in Data Factory\n",
            "   - Update Power BI query to handle new columns with `Schema.Profile = false`\n",
            "\n",
            "3. If data appears delayed:\n",
            "   - Check Cost Management export latency (up to 48 hours for EA customers)\n",
            "   - Validate Data Factory trigger is using Event Grid (not polling)\n",
            "\n",
            "**Monitoring Recommendation:**\n",
            "Set up these alerts:\n",
            "- Data Factory pipeline failures\n",
            "- Storage account blob count changes\n",
            "- Power BI refresh failures using: \n",
            "  ```kusto\n",
            "  PowerBIDatasetsWorkspace\n",
            "  | where Status == \"Failed\"\n",
            "  | project DatasetName, ErrorCode, EventDateTime\n",
            "  ```\n",
            "\n",
            "Follow these steps in sequence, and you should be able to isolate where the data pipeline is breaking - whether at export, processing, storage, or visualization layer.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "model_name = os.getenv(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key),\n",
        "        headers={\"x-ms-model-mesh-model-name\": model_name}  # Add the model name in the header\n",
        "    )\n",
        "    print(f\"‚úÖ Client initialized | Model: {client.get_model_info().model_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Initialization failed: {e}\")\n",
        "\n",
        "# Define FinOps system prompt\n",
        "finops_system_prompt = \"\"\"You are a Microsoft FinOps toolkit expert with deep knowledge of:\n",
        "\n",
        "1. FinOps Framework: The operational framework for financial management of cloud resources.\n",
        "2. Microsoft Cost Management: How to analyze, track, and allocate cloud spending.\n",
        "3. FinOps Hub Implementation: Setup, validation, and troubleshooting.\n",
        "4. Data Factory Pipelines: How they ingest and transform cost data.\n",
        "5. Power BI Cost Reports: Connection methods and optimization.\n",
        "6. VM Cost Optimization: Strategies for right-sizing, reservation, and scheduling VMs.\n",
        "\n",
        "When answering questions about FinOps hub validation, include these specific steps:\n",
        "1. Verify Cost Management exports are successful\n",
        "2. Verify Data Factory pipelines are running correctly\n",
        "3. Check msexports container for raw data files\n",
        "4. Check ingestion container for processed parquet files\n",
        "5. Validate Power BI configuration if applicable\n",
        "\n",
        "For troubleshooting Data Factory pipelines, check:\n",
        "- If msexports_ManifestAdded trigger is started\n",
        "- Whether resource providers are registered\n",
        "- For error codes in pipeline runs\n",
        "- For mapping column errors in the ETL pipeline\n",
        "\n",
        "When answering questions:\n",
        "- Provide clear, step-by-step guidance\n",
        "- Reference specific steps from Microsoft's FinOps toolkit documentation\n",
        "- Include both immediate fixes and long-term strategies\n",
        "- Consider the business impact of cost optimization\n",
        "\n",
        "Your goal is to help organizations maximize the business value derived from their cloud investment\n",
        "through the principles of FinOps: Visibility, Optimization, and Governance.\"\"\"\n",
        "\n",
        "# Create the mock documentation function\n",
        "def get_finops_documentation(query):\n",
        "    \"\"\"Mock function to provide FinOps documentation until search is configured\"\"\"\n",
        "    \n",
        "    # Dictionary mapping topics to documentation snippets\n",
        "    finops_docs = {\n",
        "        \"hub\": \"\"\"\n",
        "        # FinOps Hub Troubleshooting Guide\n",
        "        \n",
        "        ## Validate your FinOps hub deployment\n",
        "        \n",
        "        ### Step 1: Verify Cost Management exports\n",
        "        1. Go to Cost Management exports and make sure the export status is 'Successful'.\n",
        "        2. If it isn't successful, ensure you have the Cost Management resource provider registered.\n",
        "        3. File a support request with the Cost Management team to investigate further.\n",
        "        \n",
        "        ### Step 2: Verify Data Factory pipelines\n",
        "        1. From Data Factory Studio, select Monitor and confirm pipelines are running successfully.\n",
        "        2. If pipelines are failing, review the error code and message.\n",
        "        3. Compare the last run time with the time of the last export. They should be close.\n",
        "        4. Select Manage > Author > Triggers and verify the msexports_ManifestAdded trigger is started.\n",
        "        \"\"\",\n",
        "        \n",
        "        \"power bi\": \"\"\"\n",
        "        # Power BI Configuration Troubleshooting\n",
        "        \n",
        "        ## Validate your Power BI configuration\n",
        "        \n",
        "        ### Step 1: Identify your storage URL\n",
        "        Before validation, determine your connection mechanism:\n",
        "        - Cost Management connector for Power BI\n",
        "        - Cost Management exports in storage\n",
        "        - FinOps hubs\n",
        "        \n",
        "        ### Step 2: Connect Power BI to storage\n",
        "        Decide whether to connect using a user/service principal account or storage account key:\n",
        "        \n",
        "        Using a user or service principal account:\n",
        "        1. Ensure you have the Storage Blob Data Reader role explicitly assigned to the account.\n",
        "        \n",
        "        Using a SAS token:\n",
        "        1. Ensure proper permissions: Blob service, Container and Object resource types, Read and List permissions.\n",
        "        2. Ensure valid start and expiry date/time.\n",
        "        \"\"\",\n",
        "        \n",
        "        \"vm\": \"\"\"\n",
        "        # Azure VM Cost Optimization with FinOps\n",
        "        \n",
        "        ## VM Right-sizing Strategies\n",
        "        - Monitor CPU, memory, network and storage metrics to identify underutilized VMs\n",
        "        - Consider resizing to newer-generation instances with better price-performance\n",
        "        - Use Azure Advisor recommendations for right-sizing opportunities\n",
        "        \n",
        "        ## Reserved Instances\n",
        "        - Consider 1-year or 3-year commitments for predictable workloads\n",
        "        - Calculate break-even point to determine optimal reservation term\n",
        "        - Regularly review reservation utilization and exchange or return as needed\n",
        "        \n",
        "        ## Scheduling\n",
        "        - Use Azure Automation or DevTest Labs to automatically stop VMs during off-hours\n",
        "        - Create start/stop schedules that align with business hours for dev/test environments\n",
        "        \"\"\"\n",
        "    }\n",
        "    \n",
        "    # Simple keyword matching\n",
        "    result = []\n",
        "    query = query.lower()\n",
        "    for topic, content in finops_docs.items():\n",
        "        if topic in query or any(keyword in query for keyword in topic.split()):\n",
        "            result.append(content)\n",
        "    \n",
        "    # Default documentation if no specific match\n",
        "    if not result:\n",
        "        result = [finops_docs[\"hub\"]]  # Default to hub documentation\n",
        "        \n",
        "    return \"\\n\\n\".join(result)\n",
        "\n",
        "# Solve general FinOps problems\n",
        "def solve_finops_problem(problem):\n",
        "    \"\"\"Solve FinOps-related problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=f\"{problem} Please provide step-by-step analysis with actionable recommendations.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Function for VM cost analysis\n",
        "def analyze_vm_costs(vm_types, region, usage_pattern):\n",
        "    \"\"\"Analyze Azure VM costs with FinOps best practices\"\"\"\n",
        "    query = f\"\"\"As a FinOps specialist, analyze the cost-efficiency of {vm_types} in {region} with {usage_pattern} usage pattern.\n",
        "    Provide cost optimization recommendations including newer generation alternatives, reserved instances evaluation, \n",
        "    and scheduling opportunities. Include estimated monthly savings.\"\"\"\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=query)\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.5,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Troubleshoot FinOps Hub issues\n",
        "def troubleshoot_finops_hub(issue_description):\n",
        "    \"\"\"Provide troubleshooting steps for FinOps hub issues\"\"\"\n",
        "    \n",
        "    steps_template = \"\"\"Based on the Microsoft FinOps toolkit troubleshooting guide, follow these validation steps:\n",
        "\n",
        "    1. First, check if your issue matches any known error codes\n",
        "    2. Then validate each component in sequence:\n",
        "       - Cost Management exports\n",
        "       - Data Factory pipelines\n",
        "       - Storage account containers (msexports and ingestion)\n",
        "       - Power BI configuration\n",
        "    \n",
        "    For your specific issue with {issue}, I recommend:\"\"\"\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=f\"{steps_template.format(issue=issue_description)} Provide detailed troubleshooting steps.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# FinOps expert function that includes knowledge and documentation\n",
        "def finops_expert(query, include_documentation=True):\n",
        "    \"\"\"Get expert advice on FinOps toolkit questions\"\"\"\n",
        "    \n",
        "    # Optionally retrieve relevant documentation\n",
        "    docs = \"\"\n",
        "    if include_documentation:\n",
        "        docs = get_finops_documentation(query)\n",
        "        docs_context = f\"\\n\\nRelevant Microsoft FinOps documentation:\\n{docs}\\n\\n\"\n",
        "    else:\n",
        "        docs_context = \"\"\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=finops_system_prompt),\n",
        "        UserMessage(content=f\"{docs_context}User query: {query}\\n\\nProvide expert guidance on this FinOps toolkit question.\")\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        response = client.complete(\n",
        "            messages=messages,\n",
        "            model=model_name,\n",
        "            temperature=0.3,\n",
        "            max_tokens=2048\n",
        "        )\n",
        "        \n",
        "        content = response.choices[0].message.content\n",
        "        \n",
        "        # Extract reasoning if present\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            thinking = match.group(1).strip()\n",
        "            answer = match.group(2).strip()\n",
        "            return {\"thinking\": thinking, \"answer\": answer}\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Example 1: Basic FinOps question\n",
        "hub_problem = \"\"\"Our organization recently deployed a FinOps hub but we're seeing errors in our Data Factory pipelines. \n",
        "How can we troubleshoot and ensure our Cost Management exports are properly flowing into our storage account for Power BI reporting?\"\"\"\n",
        "\n",
        "print(\"üîß FinOps Challenge:\", hub_problem)\n",
        "print(\"\\nüí° Solution:\", solve_finops_problem(hub_problem))\n",
        "\n",
        "# Example 2: VM cost analysis\n",
        "vm_analysis = analyze_vm_costs(\n",
        "    \"Standard_D4s_v3 and Standard_F8s_v2\", \n",
        "    \"West Europe\", \n",
        "    \"60% during business hours, 15% during nights/weekends\"\n",
        ")\n",
        "print(\"\\nüí∞ VM Cost Analysis:\", vm_analysis)\n",
        "\n",
        "# Example 3: FinOps expert with documentation retrieval\n",
        "question = \"How do I validate my FinOps hub deployment?\"\n",
        "print(f\"\\n‚ùì Question: {question}\")\n",
        "result = finops_expert(question)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nüìù Response:\", result)\n",
        "\n",
        "# Example 4: Power BI troubleshooting\n",
        "bi_problem = \"\"\"Our Power BI reports connected to our FinOps hub storage aren't showing the latest data. \n",
        "We're using a service principal to connect to storage. What should we verify to ensure proper data refresh?\"\"\"\n",
        "print(\"\\nüîç Power BI Issue:\", bi_problem)\n",
        "print(\"\\nüîß Troubleshooting Steps:\", troubleshoot_finops_hub(bi_problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "150c569c",
      "metadata": {},
      "source": [
        "# FINOPS TOOLKIT TECHSUPPORT WITH BINGSEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7c47bf55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing DeepSeek-R1 FinOps Toolkit Expert with Tavily Search...\n",
            "‚úÖ Tavily module already installed\n",
            "‚úÖ DeepSeek-R1 client initialized | Model: gpt-4o\n",
            "‚úÖ Tavily client initialized\n",
            "\n",
            "---------- FinOps Expert with Tavily Search ----------\n",
            "\n",
            "üìù Question: I have this error message in my finops hub deployment. AccountPropertyCannotBeUpdated?\n",
            "üîç Searching for FinOps documentation about: I have this error message in my finops hub deployment. AccountPropertyCannotBeUpdated?\n",
            "‚úÖ Found relevant documentation from Tavily search\n",
            "\n",
            "üîç Answer: The error message \"AccountPropertyCannotBeUpdated\" typically occurs when attempting to update a FinOps hub deployment with a different storage account configuration than what was initially used during creation. Specifically, this error is related to the \"requireInfrastructureEncryption\" property, which can only be set once during the initial deployment of the FinOps hub and cannot be changed afterward.\n",
            "\n",
            "Here are the steps to resolve this issue:\n",
            "\n",
            "### Step-by-Step Instructions\n",
            "\n",
            "1. **Verify Current Storage Account Configuration:**\n",
            "   - Navigate to the Azure portal.\n",
            "   - Go to the **Storage accounts** service.\n",
            "   - Select the storage account used by your FinOps hub.\n",
            "   - Under the **Settings** section, click on **Configuration**.\n",
            "   - Check the **Infrastructure encryption** setting to see if it is enabled or disabled.\n",
            "\n",
            "2. **Confirm the FinOps Hub Template Configuration:**\n",
            "   - Ensure that the FinOps hub template you are using for the update has the same value for the \"requireInfrastructureEncryption\" property as the current storage account configuration.\n",
            "   - If the property was enabled during the initial deployment, it must remain enabled in the template, and vice versa.\n",
            "\n",
            "3. **Re-deploy the FinOps Hub Template:**\n",
            "   - If you did not intend to change the \"requireInfrastructureEncryption\" setting, adjust the template to match the current storage account configuration.\n",
            "   - Re-deploy the FinOps hub using the corrected template.\n",
            "\n",
            "   **Azure CLI Command:**\n",
            "   ```bash\n",
            "   az deployment group create --resource-group <ResourceGroupName> --template-file <TemplateFilePath> --parameters @<ParametersFilePath>\n",
            "   ```\n",
            "\n",
            "4. **Deploy a New FinOps Hub Instance (if necessary):**\n",
            "   - If you need to change the \"requireInfrastructureEncryption\" setting, you will need to deploy a new FinOps hub instance.\n",
            "   - This involves creating a new storage account with the desired encryption setting and reingesting all data.\n",
            "\n",
            "   **Steps to Deploy a New FinOps Hub Instance:**\n",
            "   - Create a new storage account with the desired configuration.\n",
            "   - Deploy the FinOps hub template using the new storage account.\n",
            "   - Reconfigure your data ingestion pipelines to point to the new FinOps hub instance.\n",
            "   - Reingest all historical data into the new instance.\n",
            "\n",
            "### Documentation Reference:\n",
            "- [Troubleshoot common FinOps toolkit errors](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/help/errors)\n",
            "\n",
            "### Additional Notes:\n",
            "- Ensure that all other properties in the FinOps hub template match the current configuration to avoid other potential issues.\n",
            "- Regularly back up your configuration and data to facilitate easier recovery in case of deployment issues.\n",
            "\n",
            "By following these steps, you should be able to resolve the \"AccountPropertyCannotBeUpdated\" error and successfully update or redeploy your FinOps hub.\n",
            "\n",
            "## References\n",
            "1. [Errors](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/help/errors)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# FinOps Toolkit Expert with Tavily Web Search Integration\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent.parent.parent  # Going back to the root\n",
        "load_dotenv(parent_dir / '.env')\n",
        "\n",
        "print(\"Initializing DeepSeek-R1 FinOps Toolkit Expert with Tavily Search...\")\n",
        "\n",
        "# Install requirements if needed\n",
        "try:\n",
        "    import tavily\n",
        "    print(\"‚úÖ Tavily module already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing Tavily Python SDK...\")\n",
        "    import pip\n",
        "    pip.main(['install', 'tavily-python'])\n",
        "    import tavily\n",
        "    print(\"‚úÖ Tavily module installed successfully\")\n",
        "\n",
        "try:\n",
        "    # Initialize the DeepSeek-R1 client\n",
        "    endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "    key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "    model_name = os.getenv(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "    \n",
        "    # Initialize direct chat client\n",
        "    deepseek_client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key),\n",
        "        headers={\"x-ms-model-mesh-model-name\": model_name}  # Add the model name in the header\n",
        "    )\n",
        "    print(f\"‚úÖ DeepSeek-R1 client initialized | Model: {model_name}\")\n",
        "    \n",
        "    # Initialize Tavily client - you need to set this in your .env file or directly here\n",
        "    # TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"tvly-dev-YOUR_API_KEY_HERE\")\n",
        "    TAVILY_API_KEY = input(\"Please enter your Tavily API key (starts with tvly-): \")\n",
        "    if TAVILY_API_KEY and TAVILY_API_KEY.startswith(\"tvly-\"):\n",
        "        tavily_client = tavily.TavilyClient(TAVILY_API_KEY)\n",
        "        print(\"‚úÖ Tavily client initialized\")\n",
        "    else:\n",
        "        tavily_client = None\n",
        "        print(\"‚ö†Ô∏è Invalid Tavily API key, web search will not be available\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Setup error: {e}\")\n",
        "    deepseek_client = None\n",
        "    tavily_client = None\n",
        "\n",
        "# Function to search for FinOps documentation using Tavily\n",
        "def search_finops_docs_with_tavily(query, max_results=5):\n",
        "    \"\"\"Search for FinOps documentation using Tavily search API\"\"\"\n",
        "    if not tavily_client:\n",
        "        return {\"error\": \"Tavily client not initialized\"}\n",
        "    \n",
        "    # Focus search on Microsoft FinOps documentation\n",
        "    search_query = f\"{query} Microsoft Azure FinOps hub documentation\"\n",
        "    \n",
        "    try:\n",
        "        # Execute the search\n",
        "        response = tavily_client.search(\n",
        "            query=search_query,\n",
        "            search_depth=\"advanced\",  # Use advanced for more comprehensive results\n",
        "            max_results=max_results,\n",
        "            include_answer=\"advanced\",  # Get a summarized answer\n",
        "            include_domains=[\"microsoft.com\", \"azure.com\", \"learn.microsoft.com\"]  # Focus on Microsoft docs\n",
        "        )\n",
        "        \n",
        "        # Format the results in a structured way\n",
        "        formatted_text = \"## Microsoft FinOps Documentation Search Results\\n\\n\"\n",
        "        \n",
        "        # Add the Tavily-generated answer if available\n",
        "        if \"answer\" in response and response[\"answer\"]:\n",
        "            formatted_text += f\"### Summary\\n{response['answer']}\\n\\n\"\n",
        "        \n",
        "        # Add individual search results\n",
        "        if \"results\" in response:\n",
        "            for i, result in enumerate(response[\"results\"], 1):\n",
        "                formatted_text += f\"### {i}. {result.get('title', 'Untitled')}\\n\"\n",
        "                formatted_text += f\"Source: {result.get('url', 'No URL')}\\n\"\n",
        "                formatted_text += f\"{result.get('content', 'No content available')}\\n\\n\"\n",
        "        \n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"formatted_text\": formatted_text,\n",
        "            \"raw_results\": response[\"results\"] if \"results\" in response else [],\n",
        "            \"urls\": [r.get(\"url\") for r in response.get(\"results\", []) if \"url\" in r],\n",
        "            \"answer\": response.get(\"answer\", \"\")\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Tavily search error: {str(e)}\")\n",
        "        return {\n",
        "            \"status\": \"error\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Fallback documentation if web search fails\n",
        "def get_finops_documentation(query):\n",
        "    \"\"\"Provide expert FinOps documentation when web search is unavailable\"\"\"\n",
        "    query_lower = query.lower()\n",
        "    \n",
        "    # Choose the appropriate documentation based on query keywords\n",
        "    if any(term in query_lower for term in [\"validate\", \"validation\", \"hub deployment\", \"verify\"]):\n",
        "        return finops_docs[\"hub_validation\"]\n",
        "    elif any(term in query_lower for term in [\"pipeline\", \"data factory\", \"adf\", \"export\"]):\n",
        "        return finops_docs[\"pipeline_issues\"]\n",
        "    elif any(term in query_lower for term in [\"power bi\", \"report\", \"dashboard\", \"visualization\"]):\n",
        "        return finops_docs[\"power_bi\"]\n",
        "    else:\n",
        "        # Default to hub validation\n",
        "        return finops_docs[\"hub_validation\"]\n",
        "\n",
        "# Abbreviated knowledge base - for the full version, use the comprehensive documentation from previous responses\n",
        "finops_docs = {\n",
        "    \"hub_validation\": \"\"\"\n",
        "# FinOps Hub Validation Guide\n",
        "\n",
        "## Step 1: Verify Cost Management exports\n",
        "- Navigate to Cost Management in Azure portal\n",
        "- Check the Exports section to ensure exports are completing successfully\n",
        "- Verify export schedule aligns with your requirements\n",
        "- Check if the export destination (storage account) is accessible\n",
        "\n",
        "## Step 2: Verify Data Factory pipelines\n",
        "- Open Azure Data Factory Studio\n",
        "- Check if the msexports_ManifestAdded trigger is started\n",
        "- Review the pipeline run history for any failures\n",
        "- Examine error details for failing pipeline runs\n",
        "- Verify the msexports_IngestManifestFile pipeline is running correctly\n",
        "\n",
        "## Step 3: Check storage containers\n",
        "- Access the storage account specified in your FinOps hub deployment\n",
        "- Verify the msexports container has export files from Cost Management\n",
        "- Check the ingestion container for processed parquet files\n",
        "- Validate folder structure matches expected patterns\n",
        "\n",
        "## Step 4: Validate Power BI reports\n",
        "- Open Power BI workspace containing FinOps reports\n",
        "- Check dataset refresh history for any failures\n",
        "- Verify service principal has proper access to storage\n",
        "- Test direct connection to storage from Power BI Desktop\n",
        "\"\"\",\n",
        "    \n",
        "    \"pipeline_issues\": \"\"\"\n",
        "# Data Factory Pipeline Troubleshooting\n",
        "\n",
        "## Common Issues:\n",
        "\n",
        "### 1. Permission problems\n",
        "- Ensure Data Factory managed identity has Storage Blob Data Contributor role\n",
        "- Verify managed identity has Reader access to subscription or resource group\n",
        "- Check if Cost Management resource provider is registered\n",
        "\n",
        "### 2. Trigger configuration\n",
        "- Verify msexports_ManifestAdded trigger is started\n",
        "- Check the event trigger is configured for the correct storage account\n",
        "- Validate the trigger path pattern matches your export location\n",
        "\n",
        "### 3. Data format issues\n",
        "- Inspect error logs for column mapping failures in the copy activity\n",
        "- Verify schema definitions match actual export format\n",
        "- Check for special characters or unexpected data types\n",
        "\n",
        "### 4. Networking issues\n",
        "- Confirm storage account firewall allows Azure Data Factory access\n",
        "- Verify private endpoint configurations if using private networking\n",
        "- Check NSG rules if using VNET integration\n",
        "\"\"\",\n",
        "    \n",
        "    \"power_bi\": \"\"\"\n",
        "# Power BI Connection Troubleshooting\n",
        "\n",
        "## Connection Validation Steps:\n",
        "\n",
        "### 1. Authentication verification\n",
        "- Ensure service principal has Storage Blob Data Reader role\n",
        "- Check if SAS token is valid and has appropriate permissions\n",
        "- Verify Azure AD permissions are properly assigned\n",
        "\n",
        "### 2. Connection testing\n",
        "- Test direct connection from Power BI Desktop to storage\n",
        "- Verify container and folder paths in Power BI queries\n",
        "- Check for firewalls blocking Power BI service IP ranges\n",
        "\n",
        "### 3. Gateway configuration\n",
        "- Verify on-premises data gateway if used for hybrid connections\n",
        "- Check gateway connection status in Power BI service\n",
        "- Test credentials used by the gateway\n",
        "\n",
        "### 4. Refresh scheduling\n",
        "- Review dataset refresh history for specific error messages\n",
        "- Verify refresh schedule aligns with data pipeline completion\n",
        "- Check if credential expiration is causing refresh failures\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# FinOps expert function with Tavily search\n",
        "def finops_expert_with_tavily(query):\n",
        "    \"\"\"Get expert FinOps advice using DeepSeek-R1 and Tavily search\"\"\"\n",
        "    \n",
        "    if not deepseek_client:\n",
        "        return \"Error: DeepSeek-R1 client not initialized. Please check your configuration.\"\n",
        "    \n",
        "    try:\n",
        "        print(f\"üîç Searching for FinOps documentation about: {query}\")\n",
        "        \n",
        "        # Try to get documentation from Tavily search if client is available\n",
        "        if tavily_client:\n",
        "            search_result = search_finops_docs_with_tavily(query)\n",
        "            \n",
        "            # Prepare context based on search results\n",
        "            if search_result.get('status') == 'success':\n",
        "                print(\"‚úÖ Found relevant documentation from Tavily search\")\n",
        "                context = search_result['formatted_text']\n",
        "                urls = search_result.get('urls', [])\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Web search failed: {search_result.get('error', 'Unknown error')}\")\n",
        "                print(\"Using built-in FinOps documentation instead\")\n",
        "                context = get_finops_documentation(query)\n",
        "                urls = []\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Tavily client not available, using built-in documentation\")\n",
        "            context = get_finops_documentation(query)\n",
        "            urls = []\n",
        "        \n",
        "        # Define the finops system prompt\n",
        "        finops_system_prompt = \"\"\"You are a Microsoft FinOps toolkit expert with deep knowledge of:\n",
        "        \n",
        "        1. FinOps Framework: The operational framework for financial management of cloud resources.\n",
        "        2. Microsoft Cost Management: How to analyze, track, and allocate cloud spending.\n",
        "        3. FinOps Hub Implementation: Setup, validation, and troubleshooting.\n",
        "        4. Data Factory Pipelines: How they ingest and transform cost data.\n",
        "        5. Power BI Cost Reports: Connection methods and optimization.\n",
        "        6. VM Cost Optimization: Strategies for right-sizing, reservation, and scheduling VMs.\n",
        "        \n",
        "        When answering questions about FinOps hub validation, include these specific steps:\n",
        "        1. Verify Cost Management exports are successful\n",
        "        2. Verify Data Factory pipelines are running correctly\n",
        "        3. Check msexports container for raw data files\n",
        "        4. Check ingestion container for processed parquet files\n",
        "        5. Validate Power BI configuration if applicable\n",
        "        \n",
        "        For Data Factory pipeline troubleshooting, check:\n",
        "        - If msexports_ManifestAdded trigger is started\n",
        "        - Whether all necessary resource providers are registered\n",
        "        - For specific error codes in pipeline run history\n",
        "        - For column mapping errors in the ETL pipeline\n",
        "\n",
        "        When answering questions:\n",
        "        - Provide clear, step-by-step guidance\n",
        "        - Include specific Azure portal navigation steps\n",
        "        - Mention PowerShell or Azure CLI commands where applicable\n",
        "        - Reference the documentation provided in the context\n",
        "        - Include both immediate fixes and long-term strategies\n",
        "        \"\"\"\n",
        "        \n",
        "        # Add the search results as context\n",
        "        user_message = f\"\"\"\n",
        "        Based on Microsoft's FinOps documentation:\n",
        "        \n",
        "        {context}\n",
        "        \n",
        "        User query: {query}\n",
        "        \n",
        "        Please provide expert guidance on this FinOps toolkit question based on the documentation.\n",
        "        Include specific, step-by-step instructions and reference the documentation where relevant.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Get DeepSeek reasoning\n",
        "        response = deepseek_client.complete(\n",
        "            messages=[\n",
        "                SystemMessage(content=finops_system_prompt),\n",
        "                UserMessage(content=user_message)\n",
        "            ],\n",
        "            model=model_name,\n",
        "            temperature=0.3,\n",
        "            max_tokens=2048\n",
        "        )\n",
        "        \n",
        "        content = response.choices[0].message.content\n",
        "        \n",
        "        # Add references to the content if URLs were found\n",
        "        if urls:\n",
        "            content += \"\\n\\n## References\\n\"\n",
        "            for i, url in enumerate(urls, 1):\n",
        "                content += f\"{i}. [{url.split('/')[-1].replace('-', ' ').title()}]({url})\\n\"\n",
        "        \n",
        "        return content\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n---------- FinOps Expert with Tavily Search ----------\")\n",
        "finops_questions = [\n",
        "    \"I have this error message in my finops hub deployment. AccountPropertyCannotBeUpdated?\",\n",
        "\n",
        "]\n",
        "\n",
        "# Test with the first question\n",
        "print(f\"\\nüìù Question: {finops_questions[0]}\")\n",
        "answer = finops_expert_with_tavily(finops_questions[0])\n",
        "print(f\"\\nüîç Answer: {answer}\")\n",
        "\n",
        "# Uncomment to test all questions\n",
        "# for question in finops_questions:\n",
        "#     print(f\"\\nüìù Question: {question}\")\n",
        "#     answer = finops_expert_with_tavily(question)\n",
        "#     print(f\"\\nüîç Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9f7a8c",
      "metadata": {},
      "source": [
        "## 4. Best Practices & Considerations\n",
        "\n",
        "1. **Reasoning Handling**: Use regex to separate <think> content from final answers\n",
        "2. **Safety**: Built-in content filtering - handle HttpResponseError for violations\n",
        "3. **Performance**:\n",
        "   - Max tokens: 4096\n",
        "   - Rate limit: 200K tokens/minute\n",
        "4. **Cost**: Pay-as-you-go with serverless deployment\n",
        "5. **Streaming**: Implement response streaming for long completions\n",
        "\n",
        "```python\n",
        "# Streaming example\n",
        "response = client.complete(..., stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "- Leverage 128K context for detailed analysis\n",
        "- Extract reasoning steps for debugging/analysis\n",
        "- Combine with Azure AI Content Safety for production\n",
        "- Monitor token usage via response.usage\n",
        "\n",
        "> Always validate model outputs for critical applications!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
