#!/usr/bin/env python
# evaluate_answer_quality.py
# --------------------------
# Evaluates the quality of a FinOps answer and suggests improvements
# Can be used to validate answers generated by any source
# --------------------------

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from azure.core.credentials import AzureKeyCredential
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage

# Load environment variables - find .env file by searching up the directory tree
def find_dotenv():
    """Find .env file by searching up the directory tree"""
    current_path = Path().absolute()
    
    # Try current directory and up to 3 levels up
    for _ in range(4):
        env_path = current_path / '.env'
        if env_path.exists():
            print(f"Found .env file at: {env_path}")
            return env_path
        current_path = current_path.parent
    
    # Default to current directory .env if not found
    print("Could not find .env file, defaulting to current directory")
    return '.env'

# Load environment variables
dotenv_path = find_dotenv()
load_dotenv(dotenv_path)

print("üîç FinOps Answer Quality Evaluator")
print("=================================")

# Helper for getting environment variables with fallbacks
def get_env_var(name, fallback_names=None, default=None):
    """Get environment variable with fallbacks and defaults"""
    value = os.getenv(name)
    
    # Try fallbacks if provided and main value is not set
    if not value and fallback_names:
        for fallback in fallback_names:
            value = os.getenv(fallback)
            if value:
                print(f"Using {fallback} as fallback for {name}")
                break
    
    # Use default if still not found
    if not value and default:
        print(f"Using default value for {name}")
        value = default
        
    return value

# Get Azure credentials
endpoint = get_env_var("AZURE_INFERENCE_ENDPOINT")
key = get_env_var("AZURE_INFERENCE_KEY")
model_name = get_env_var("MODEL_NAME", default="gpt-4o")

if not endpoint or not key:
    print("‚ùå Error: Missing Azure credentials")
    print("Please set AZURE_INFERENCE_ENDPOINT and AZURE_INFERENCE_KEY in your .env file")
    sys.exit(1)

# Initialize client for evaluation
try:
    print(f"\nInitializing evaluation client with model: {model_name}")
    eval_client = ChatCompletionsClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(key),
        temperature=0.1,  # Low temperature for more consistent evaluation
        top_p=0.1      # Slightly higher top_p for more nuanced evaluation
    )
    print("‚úÖ Evaluation client initialized successfully")
except Exception as e:
    print(f"‚ùå Error initializing evaluation client: {str(e)}")
    sys.exit(1)

# Function to evaluate the quality of an answer
def evaluate_answer_quality(question, answer):
    """
    Evaluate the quality of a FinOps answer
    
    Parameters:
    -----------
    question : str
        The original question
    answer : str
        The answer to evaluate
        
    Returns:
    --------
    dict
        Evaluation results
    """
    try:
        print("\nüîç Evaluating answer quality...")
        
        # System prompt for the evaluation agent
        evaluation_system_prompt = """You are a Critical FinOps Answer Evaluator with expertise in Microsoft Azure Cost Management, FinOps principles, and technical documentation.

        Your task is to critically evaluate a FinOps-related answer for quality, accuracy, and usefulness. You must be thorough and honest in your assessment, focusing on whether the answer truly helps the user solve their problem.

        ## Evaluation Criteria:

        1. Accuracy (30%):
           - Is all technical information correct and up-to-date?
           - Are specific Azure features described correctly?
           - Are any factual errors present?
           - Are all recommendations aligned with Microsoft best practices?

        2. Completeness (25%):
           - Does the answer address all aspects of the question?
           - Are there missing steps or considerations?
           - Is the level of detail appropriate for the complexity of the question?
           - Are edge cases or exceptions handled?

        3. Clarity & Organization (20%):
           - Is the answer well-structured with logical flow?
           - Are section headings used appropriately?
           - Are complex concepts explained clearly?
           - Is technical jargon defined when necessary?

        4. Actionability (15%):
           - Does the answer provide clear steps the user can follow?
           - Are validation steps included after each action?
           - Does it explain what success looks like?
           - Is there guidance for handling failures or errors?

        5. Evidence & Citations (10%):
           - Are claims supported by documentation links?
           - Are Microsoft official sources cited?
           - Is it clear which guidance is official vs. best practice?
           - Are all relevant references included?

        ## Your Response Format:

        Provide your evaluation as follows:

        1. Overall Score (1-10): Give a single score that represents the overall quality.
           - 1-3: Poor, contains major issues
           - 4-6: Adequate but needs significant improvement
           - 7-8: Good with minor issues
           - 9-10: Excellent, comprehensive and accurate

        2. Strengths: List 2-3 specific strengths of the answer.

        3. Weaknesses: List 2-4 specific issues or weaknesses, ordered by importance.

        4. Improvement Suggestions: Provide 2-3 specific, actionable suggestions to improve the answer.

        5. Evaluation Summary: A 2-3 sentence summary of your overall assessment.

        Be specific and reference actual content from the answer in your evaluation. Focus on substance over style, and be particularly vigilant for technical inaccuracies or omissions.
        """
        
        # Prepare the message for evaluation
        user_message = f"""
        # Original Question
        {question}

        # Generated Answer
        {answer}

        # Evaluation Instructions
        Please evaluate the above answer according to the criteria in your instructions.
        Be thorough but fair in your assessment.
        If you identify any factual errors or critical omissions, highlight them specifically.
        """
        
        # Get evaluation from the model
        response = eval_client.complete(
            messages=[
                SystemMessage(content=evaluation_system_prompt),
                UserMessage(content=user_message)
            ],
            model=model_name,
            max_tokens=2000  # Sufficient tokens for detailed evaluation
        )
        
        evaluation_text = response.choices[0].message.content
        
        # Parse the evaluation text to extract structured information
        # This is a simple parsing implementation - could be more robust
        overall_score = None
        strengths = []
        weaknesses = []
        improvement_suggestions = []
        evaluation_summary = ""
        
        # Simple parsing based on section headers
        sections = {
            "Overall Score": [],
            "Strengths": [],
            "Weaknesses": [],
            "Improvement Suggestions": [],
            "Evaluation Summary": []
        }
        
        current_section = None
        for line in evaluation_text.split('\n'):
            line = line.strip()
            if not line:
                continue
                
            # Check if this line is a section header
            for section_name in sections.keys():
                if section_name in line or line.lower().startswith(section_name.lower()):
                    current_section = section_name
                    break
            
            # Add content to current section
            if current_section and not line.startswith(current_section):
                sections[current_section].append(line)
        
        # Extract overall score
        for line in sections["Overall Score"]:
            # Try to find a number between 1-10
            import re
            score_matches = re.findall(r'\b([1-9]|10)\b', line)
            if score_matches:
                overall_score = int(score_matches[0])
                break
        
        # Extract lists items, typically starting with "- " or "1. "
        for line in sections["Strengths"]:
            if line.startswith(('-', '‚Ä¢', '*', '1.', '2.', '3.')):
                strengths.append(line.lstrip('- ‚Ä¢*123. '))
        
        for line in sections["Weaknesses"]:
            if line.startswith(('-', '‚Ä¢', '*', '1.', '2.', '3.', '4.')):
                weaknesses.append(line.lstrip('- ‚Ä¢*1234. '))
        
        for line in sections["Improvement Suggestions"]:
            if line.startswith(('-', '‚Ä¢', '*', '1.', '2.', '3.')):
                improvement_suggestions.append(line.lstrip('- ‚Ä¢*123. '))
        
        # Join the evaluation summary lines
        evaluation_summary = ' '.join(sections["Evaluation Summary"])
        
        # If no overall score was found, estimate one based on the evaluation
        if overall_score is None:
            # Count positive vs negative terms as a fallback
            positive_terms = ['excellent', 'good', 'thorough', 'comprehensive', 'accurate', 'clear']
            negative_terms = ['poor', 'inadequate', 'missing', 'error', 'incorrect', 'unclear', 'lacks']
            
            positive_count = sum(1 for term in positive_terms if term in evaluation_text.lower())
            negative_count = sum(1 for term in negative_terms if term in evaluation_text.lower())
            
            if positive_count > negative_count * 2:
                overall_score = 8  # Mostly positive
            elif positive_count > negative_count:
                overall_score = 6  # More positive than negative
            elif negative_count > positive_count * 2:
                overall_score = 3  # Mostly negative
            else:
                overall_score = 5  # Mixed
        
        # Prepare evaluation results
        evaluation_results = {
            "overall_score": overall_score,
            "strengths": strengths,
            "weaknesses": weaknesses,
            "improvement_suggestions": improvement_suggestions,
            "evaluation_summary": evaluation_summary,
            "raw_evaluation": evaluation_text
        }
        
        return evaluation_results
        
    except Exception as e:
        print(f"‚ùå Error in quality evaluation: {str(e)}")
        return {
            "overall_score": None,
            "evaluation_summary": f"Error during evaluation: {str(e)}",
            "error": str(e)
        }

def improve_answer_if_needed(question, answer, evaluation_results):
    """
    Automatically improve an answer based on evaluation results
    
    Parameters:
    -----------
    question : str
        The original question
    answer : str
        The original answer
    evaluation_results : dict
        Results from evaluate_answer_quality function
        
    Returns:
    --------
    str
        Improved answer
    """
    try:
        print("\nüîß Generating improvements based on evaluation...")
        
        # Extract improvement points
        weaknesses = evaluation_results.get("weaknesses", [])
        suggestions = evaluation_results.get("improvement_suggestions", [])
        
        improvement_points = []
        improvement_points.extend(weaknesses)
        improvement_points.extend(suggestions)
        
        # Avoid duplicates in improvement points
        unique_points = []
        for point in improvement_points:
            if point not in unique_points:
                unique_points.append(point)
        
        # Format the improvement points
        improvement_guidance = ""
        if unique_points:
            improvement_guidance = "Focus on these specific improvements:\n"
            for i, point in enumerate(unique_points, 1):
                improvement_guidance += f"{i}. {point}\n"
        
        # System prompt for the improvement agent
        improvement_system_prompt = """You are a FinOps Answer Improvement Agent with expertise in Microsoft Azure Cost Management, FinOps principles, and technical documentation.

        Your task is to improve an existing answer based on specific evaluation feedback. You must maintain all factually correct information while addressing the identified weaknesses and suggested improvements.

        ## Improvement Guidelines:

        1. Address all identified weaknesses and suggestions in the evaluation
        2. Maintain all factually correct and useful content from the original answer
        3. Preserve all citations, links and references from the original answer
        4. Improve organization and clarity where needed
        5. Add any missing critical information that should have been included
        6. Ensure step-by-step instructions are clear and include validation steps
        7. Format code blocks and commands properly
        8. Use appropriate section headings for better organization

        ## DO NOT:
        - Do not contradict factually correct information in the original answer
        - Do not remove useful technical details
        - Do not invent technical information not supported by evidence
        - Do not drastically change the structure if it's already logical

        Deliver a complete, improved answer that addresses all the evaluation feedback while preserving the strengths of the original answer.
        """
        
        # Prepare the message for improvement
        user_message = f"""
        # Original Question
        {question}

        # Original Answer
        {answer}

        # Evaluation Feedback
        Overall Score: {evaluation_results.get('overall_score')}/10

        Weaknesses:
        {chr(10).join('- ' + w for w in weaknesses)}

        Improvement Suggestions:
        {chr(10).join('- ' + s for s in suggestions)}

        {improvement_guidance}

        # Improvement Instructions
        Please provide a fully improved version of the answer that addresses all the weaknesses and follows the improvement suggestions.
        Return a complete, stand-alone answer that the user can use without needing to refer to the original.
        """
        
        # Get improvement from the model
        response = eval_client.complete(
            messages=[
                SystemMessage(content=improvement_system_prompt),
                UserMessage(content=user_message)
            ],
            model=model_name,
            temperature=0.2,  # Low temperature for focused improvements
            max_tokens=4000   # More tokens for comprehensive improvements
        )
        
        improved_answer = response.choices[0].message.content
        return improved_answer
        
    except Exception as e:
        print(f"‚ùå Error improving answer: {str(e)}")
        return answer  # Return original answer if improvement fails

# Main function
def main():
    """Main function"""
    # Determine how to get question and answer
    print("\nHow would you like to input the question and answer?")
    print("1. Enter them directly")
    print("2. Load from a file")
    choice = input("Enter your choice (1-2): ")
    
    question = ""
    answer = ""
    
    if choice == "2":
        filepath = input("\nEnter the path to the file containing the question and answer: ")
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Try to extract question and answer
            if '# ' in content and '\n\n' in content:
                # Assume markdown format with heading
                parts = content.split('\n\n', 1)
                question = parts[0].replace('# ', '')
                answer = parts[1]
            else:
                print("Could not automatically parse file format.")
                question = input("Enter the question: ")
                answer = content
        except Exception as e:
            print(f"‚ùå Error reading file: {str(e)}")
            question = input("Enter the question: ")
            answer = input("Enter the answer (multi-line input, type 'END' on a new line to finish):\n")
            content = []
            line = input()
            while line != "END":
                content.append(line)
                line = input()
            answer = '\n'.join(content)
    else:
        question = input("\nEnter the question: ")
        print("\nEnter the answer (multi-line input, type 'END' on a new line to finish):")
        content = []
        line = input()
        while line != "END":
            content.append(line)
            line = input()
        answer = '\n'.join(content)
    
    # Evaluate answer quality
    evaluation_results = evaluate_answer_quality(question, answer)
    
    # Print evaluation results
    score = evaluation_results.get("overall_score")
    if score:
        # Determine emoji based on score
        score_emoji = "üü¢" if score >= 7 else "üü†" if score >= 5 else "üî¥"
        
        print(f"\n{score_emoji} Quality Evaluation Score: {score}/10")
        print(f"Evaluation Summary: {evaluation_results.get('evaluation_summary', 'No summary provided')}")
        
        # Print strengths
        if evaluation_results.get("strengths"):
            print("\nStrengths:")
            for i, strength in enumerate(evaluation_results["strengths"], 1):
                print(f"‚úì {i}. {strength}")
        
        # Print weaknesses
        if evaluation_results.get("weaknesses"):
            print("\nWeaknesses:")
            for i, weakness in enumerate(evaluation_results["weaknesses"], 1):
                print(f"‚úó {i}. {weakness}")
        
        # Print improvement suggestions
        if evaluation_results.get("improvement_suggestions"):
            print("\nImprovement Suggestions:")
            for i, suggestion in enumerate(evaluation_results["improvement_suggestions"], 1):
                print(f"‚Ä¢ {i}. {suggestion}")
    else:
        print(f"\n‚ùå Evaluation failed: {evaluation_results.get('evaluation_summary', 'Unknown error')}")
        return
    
    # Ask if user wants to see the raw evaluation
    if input("\nWould you like to see the full raw evaluation? (y/n): ").lower() == 'y':
        print("\nRaw Evaluation:")
        print("="*80)
        print(evaluation_results.get("raw_evaluation", "No raw evaluation available"))
        print("="*80)
    
    # Ask if user wants to improve the answer
    if score < 7 and input("\nWould you like to generate an improved version of the answer? (y/n): ").lower() == 'y':
        improved_answer = improve_answer_if_needed(question, answer, evaluation_results)
        
        print("\n‚úÖ Improved Answer:")
        print("="*80)
        print(improved_answer)
        print("="*80)
        
        # Save improved answer to file
        if input("\nWould you like to save the improved answer to a file? (y/n): ").lower() == 'y':
            # Create safe filename from question
            import re
            safe_filename = re.sub(r'[^\w\s-]', '', question).strip().lower()
            safe_filename = re.sub(r'[-\s]+', '-', safe_filename)
            safe_filename = safe_filename[:50]  # Limit length
            
            filename = f"improved_finops_answer_{safe_filename}.md"
            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(f"# {question}\n\n")
                    f.write(improved_answer)
                print(f"\n‚úÖ Improved answer saved to {filename}")
            except Exception as e:
                print(f"\n‚ùå Error saving file: {str(e)}")

if __name__ == "__main__":
    main()
    print("\nThank you for using the FinOps Answer Quality Evaluator!") 