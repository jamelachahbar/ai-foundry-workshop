{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
        "\n",
        "**DeepSeek-R1** is a state-of-the-art reasoning model combining reinforcement learning and supervised fine-tuning, excelling at complex reasoning tasks with 37B active parameters and 128K context window.\n",
        "\n",
        "In this notebook, you'll learn to:\n",
        "1. **Initialize** the ChatCompletionsClient for Azure serverless endpoints\n",
        "2. **Chat** with DeepSeek-R1 using reasoning extraction\n",
        "3. **Implement** a travel planning example with step-by-step reasoning\n",
        "4. **Leverage** the 128K context window for complex scenarios\n",
        "\n",
        "## Why DeepSeek-R1?\n",
        "- **Advanced Reasoning**: Specializes in chain-of-thought problem solving\n",
        "- **Massive Context**: 128K token window for detailed analysis\n",
        "- **Efficient Architecture**: 37B active parameters from 671B total\n",
        "- **Safety Integrated**: Built-in content filtering capabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication\n",
        "\n",
        "Required packages:\n",
        "- `azure-ai-inference`: For chat completions\n",
        "- `python-dotenv`: For environment variables\n",
        "\n",
        ".env file requirements:\n",
        "```bash\n",
        "AZURE_INFERENCE_ENDPOINT=<your-endpoint-url>\n",
        "AZURE_INFERENCE_KEY=<your-api-key>\n",
        "MODEL_NAME=DeepSeek-R1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Client initialized | Model: deepseek-r1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "model_name = os.getenv(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key),\n",
        "        headers={\"x-ms-model-mesh-model-name\": model_name}  # Add the model name in the header\n",
        "    )\n",
        "    print(\"‚úÖ Client initialized | Model:\", client.get_model_info().model_name)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Initialization failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. Intelligent Travel Planning ‚úàÔ∏è\n",
        "\n",
        "Demonstrate DeepSeek-R1's reasoning capabilities for trip planning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üó∫Ô∏è Query: Plan a 5-day cultural trip to Kyoto in April\n",
            "\n",
            "üìù Response: <think>\n",
            "Okay, I need to plan a 5-day cultural trip to Kyoto in April, including hidden gems and safety considerations. Let me start by thinking about the key elements involved here. \n",
            "\n",
            "First, Kyoto is famous for its temples, gardens, and traditional culture. April is cherry blossom season, which is a big draw, but also means it'll be crowded. So I should balance popular spots with less crowded \"hidden gems\" to give a more authentic experience. Safety is another aspect‚Äîmaybe things like staying hydrated, being cautious in crowded areas, and respecting local customs.\n",
            "\n",
            "Day 1: Arrival and orientation. The user might arrive at Kansai International Airport. Need to suggest transportation to Kyoto, maybe the Haruka Express. Then check into accommodation. Possibly a ryokan for cultural immersion. Evening stroll in Gion? But that's a popular area. Maybe include a less crowded spot nearby. Safety tip for first day: navigating public transport, maybe getting a Suica card.\n",
            "\n",
            "Day 2: Temples and gardens. Must-see places like Kinkaku-ji (Golden Pavilion) and Ryoan-ji. But those are very crowded. Hidden gem could be something like Tofuku-ji Temple's Hojo Garden, which is less crowded. Also, maybe a lesser-known temple. Safety: comfortable shoes, staying on paths. April weather is mild but maybe rain, so umbrella.\n",
            "\n",
            "Day 3: Arashiyama area. Bamboo Grove is a must, but early morning to avoid crowds. Tenryu-ji Temple there. Hidden gem: Otagi Nenbutsu-ji Temple with the many statues. Also, maybe the Saga-Toriimoto Preserved Street. Lunch in a local restaurant. Safety: watch for bicycles in Arashiyama, stay hydrated.\n",
            "\n",
            "Day 4: Day trip to Uji. Famous for Byodo-in Temple and matcha. Maybe include the Ujigami Shrine, a UNESCO site but less crowded. Stroll along the Uji River. Safety: train schedules, maybe JR pass usage. Return to Kyoto in the evening.\n",
            "\n",
            "Day 5: Fushimi Inari Taisha. Early morning hike to avoid crowds. Explore the trails beyond the main path. Hidden gem: Tofuku-ji Temple (if not already visited) or another nearby spot. Last-minute shopping in Kyoto Station area. Safety: proper footwear for hiking, be aware of monkeys on the trails.\n",
            "\n",
            "Other considerations: Transportation passes like the JR Pass or Kyoto City Bus Day Pass. Food recommendations: kaiseki, yudofu, matcha treats. Etiquette: removing shoes, quiet in temples. Emergency info: police number, location of hospitals. Maybe include a day for Nara as an alternative, but user specified Kyoto so stay within. Also, check if any special events in April like Miyako Odori dance performance in Gion.\n",
            "\n",
            "Wait, on Day 2, Tofuku-ji was mentioned. Did I mention it twice? Need to check. Also, maybe include Philosopher's Path, which is nice in April with cherry blossoms but can be crowded. However, maybe a hidden part of it or nearby temples like Honen-in or Anraku-ji which are quieter.\n",
            "\n",
            "Safety considerations: Pickpockets in crowded areas, though not common in Japan, still a consideration. Also, respecting COVID guidelines if any are in place. Maybe mention having cash as some smaller places don't take cards. Language tips: basic phrases, using translation apps.\n",
            "\n",
            "Hidden gems: Maybe the Shogunzuka Mound and Seiryuden Hall for a panoramic view of Kyoto. Or Kurama-dera Temple in the northern mountains, but that might be a bit far. Alternatively, the Kyoto Imperial Park, which is spacious and less crowded. Nishiki Market is popular but can be suggested for food with tips on best times to visit.\n",
            "\n",
            "I need to structure each day with morning, afternoon, evening, plus hidden gems and safety tips each day. Also overall safety and cultural tips at the end. Ensure the plan is logical in terms of locations grouped by area to minimize travel time. For example, Day 2 could be Eastern Kyoto (Higashiyama), Day 3 Western (Arashiyama), etc. Check if the days are geographically efficient.\n",
            "\n",
            "Also, think about lunch spots that are local favorites but not tourist traps. Maybe recommend specific restaurants or types of food. For example, in Arashiyama, Shigetsu for vegetarian temple cuisine. Or at Fushimi Inari, a nearby inari sushi place.\n",
            "\n",
            "Wait, when mentioning Uji, the Byodo-in Temple's Phoenix Hall is iconic. Make sure to highlight that. Also, the tea houses in Uji for matcha experience.\n",
            "\n",
            "Need to ensure days are not too packed, allow time for immersion. Maybe include some downtime or leisurely walks. Also, transport between locations‚Äîusing buses, trains, taxis? Kyoto's bus system is extensive but can be slow due to traffic. Maybe suggest using subways or\n"
          ]
        }
      ],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a travel expert. Provide detailed plans with rationale.\"),\n",
        "        UserMessage(content=f\"{query} Include hidden gems and safety considerations.\")\n",
        "    ]\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=messages,\n",
        "        model=model_name,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract reasoning if present\n",
        "    if show_thinking:\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "    return content\n",
        "\n",
        "# Example usage\n",
        "query = \"Plan a 5-day cultural trip to Kyoto in April\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"üó∫Ô∏è Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nüìù Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. Technical Problem Solving üíª\n",
        "\n",
        "Showcase coding/optimization capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bfa075d",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí∞ FinOps Challenge: As a FinOps specialist, analyze how to optimize costs for an Azure environment with 50 VMs (mix of D-series and F-series) \n",
            "running in West Europe. Most VMs are running 24/7, but actual usage patterns show 60% utilization during business hours and 15% during nights/weekends. \n",
            "What cost-saving strategies would you recommend, including newer VM generations that might offer better performance/cost ratio?\n",
            "\n",
            "üí° Optimization Strategy: To optimize costs for an Azure environment with 50 VMs (a mix of D-series and F-series) running in West Europe, we need to consider several strategies. Given the usage patterns (60% utilization during business hours and 15% during nights/weekends), we can leverage various Azure features and best practices to reduce costs. Here‚Äôs a step-by-step analysis and recommendations:\n",
            "\n",
            "### 1. **Analyze VM Usage and Right-Size VMs**\n",
            "- **Current Utilization Analysis**: With 60% utilization during business hours and 15% during nights/weekends, it‚Äôs clear that the VMs are underutilized outside of peak hours.\n",
            "- **Right-Sizing**: Use Azure Advisor and Azure Monitor to analyze the performance metrics of your VMs. Identify VMs that are over-provisioned and can be resized to smaller instances. For example, if a D4s_v3 VM is consistently underutilized, consider resizing it to a D2s_v3.\n",
            "\n",
            "### 2. **Leverage Auto-Scaling and VMSS (Virtual Machine Scale Sets)**\n",
            "- **Auto-Scaling**: Implement auto-scaling policies to adjust the number of running VMs based on actual demand. This can be particularly useful if your workload can tolerate scaling in and out. Use Azure Automation or Azure Functions to schedule scaling actions based on your business hours.\n",
            "- **VMSS**: Use Virtual Machine Scale Sets to automatically adjust the number of VMs. This allows you to scale out during peak hours and scale in during off-peak hours.\n",
            "\n",
            "### 3. **Evaluate and Migrate to Newer VM Generations**\n",
            "- **Newer VM Generations**: Azure frequently releases new VM series that offer better performance and cost efficiency. Evaluate the latest Dv5 and Fsv2 series, which may provide better performance at a lower cost compared to older D-series and F-series VMs.\n",
            "- **Performance/Cost Analysis**: Conduct a performance and cost analysis to compare the current VMs with the newer generations. For example, the Dv5 series might offer better performance per dollar compared to the Dv3 series.\n",
            "\n",
            "### 4. **Utilize Reserved Instances (RIs)**\n",
            "- **Reserved Instances**: For VMs that need to run 24/7, consider purchasing Reserved Instances. RIs can provide significant discounts (up to 72%) compared to pay-as-you-go pricing. Evaluate your long-term VM usage patterns and commit to 1-year or 3-year RIs for consistent workloads.\n",
            "- **Hybrid Benefit**: If you have existing Windows Server licenses with Software Assurance, leverage the Azure Hybrid Benefit to reduce costs further.\n",
            "\n",
            "### 5. **Implement Dev/Test Pricing**\n",
            "- **Dev/Test Environments**: If some of the VMs are used for development and testing, consider moving them to Azure Dev/Test pricing, which offers substantial discounts for non-production workloads.\n",
            "\n",
            "### 6. **Optimize Storage Costs**\n",
            "- **Managed Disks**: Ensure that you are using the appropriate type of managed disks (Standard HDD, Standard SSD, Premium SSD) based on your performance requirements. For less critical VMs, consider using Standard HDDs to save costs.\n",
            "- **Snapshots and Backups**: Regularly review and clean up old snapshots and backups that are no longer needed to avoid unnecessary storage costs.\n",
            "\n",
            "### 7. **Use Azure Spot VMs**\n",
            "- **Spot VMs**: For workloads that are flexible and can tolerate interruptions, consider using Azure Spot VMs. These VMs can be significantly cheaper but may be evicted when Azure needs the capacity.\n",
            "\n",
            "### 8. **Monitor and Optimize Networking Costs**\n",
            "- **Networking Costs**: Review and optimize networking costs, including data transfer and load balancer usage. Ensure that VMs within the same region are using the same virtual network to minimize data transfer costs.\n",
            "\n",
            "### 9. **Continuous Monitoring and Optimization**\n",
            "- **Azure Cost Management**: Use Azure Cost Management and Billing to continuously monitor your spending and identify cost-saving opportunities.\n",
            "- **Alerts and Budgets**: Set up cost alerts and budgets to ensure that you are notified of any unexpected increases in spending.\n",
            "\n",
            "### Key Recommendations Summary:\n",
            "1. **Right-Size VMs**: Resize over-provisioned VMs to smaller instances.\n",
            "2. **Auto-Scaling**: Implement auto-scaling to adjust VM count based on demand.\n",
            "3. **Newer VM Generations**: Evaluate and migrate to Dv5 and Fsv2 series for better performance/cost ratio.\n",
            "4. **Reserved Instances**: Purchase RIs for consistent workloads to save up to 72%.\n",
            "5. **Dev/Test Pricing**: Use Dev/Test pricing for non-production environments.\n",
            "6. **Optimize Storage**: Use appropriate managed disk types and clean up old snapshots.\n",
            "7. **Spot VMs**: Use Spot VMs for interruptible workloads.\n",
            "8. **Networking Costs**: Optimize data transfer and load balancer usage.\n",
            "9. **Continuous Monitoring**: Use Azure Cost Management to monitor and optimize costs.\n",
            "\n",
            "By implementing these strategies, you can significantly reduce your Azure costs while maintaining the necessary performance and availability for your workloads.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\n# Example 2: Reserved Instance Analysis\\nproblem_ri = '''Analyze the cost-benefit of committing to 3-year reserved instances for 20 Standard_D4s_v3 VMs \\ncurrently on pay-as-you-go pricing in West Europe. Include ROI calculation and break-even analysis.'''\\n\\n# Example 3: VM Right-sizing\\nproblem_rightsize = '''Our monitoring shows our Standard_F8s_v2 VMs are consistently using less than 30% CPU and 40% memory.\\nRecommend right-sizing options that maintain the same number of NICs but optimize costs.'''\\n\\n# Example 4: Hybrid Azure/On-Premises Strategy\\nproblem_hybrid = '''We currently run 35 VMs in Azure and 40 on-premises servers nearing end-of-life.\\nWhat would be the most cost-effective migration strategy considering TCO for the next 5 years?'''\\n\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=\"You are a FinOps expert specializing in Azure cost optimization. Provide detailed, actionable advice.\"),\n",
        "            UserMessage(content=f\"{problem} Please reason step by step and highlight key recommendations.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# FinOps for Azure optimization examples\n",
        "problem = \"\"\"As a FinOps specialist, analyze how to optimize costs for an Azure environment with 50 VMs (mix of D-series and F-series) \n",
        "running in West Europe. Most VMs are running 24/7, but actual usage patterns show 60% utilization during business hours and 15% during nights/weekends. \n",
        "What cost-saving strategies would you recommend, including newer VM generations that might offer better performance/cost ratio?\"\"\"\n",
        "\n",
        "print(\"üí∞ FinOps Challenge:\", problem)\n",
        "print(\"\\nüí° Optimization Strategy:\", solve_technical_problem(problem))\n",
        "\n",
        "# Additional FinOps examples (commented for future use)\n",
        "\"\"\"\n",
        "# Example 2: Reserved Instance Analysis\n",
        "problem_ri = '''Analyze the cost-benefit of committing to 3-year reserved instances for 20 Standard_D4s_v3 VMs \n",
        "currently on pay-as-you-go pricing in West Europe. Include ROI calculation and break-even analysis.'''\n",
        "\n",
        "# Example 3: VM Right-sizing\n",
        "problem_rightsize = '''Our monitoring shows our Standard_F8s_v2 VMs are consistently using less than 30% CPU and 40% memory.\n",
        "Recommend right-sizing options that maintain the same number of NICs but optimize costs.'''\n",
        "\n",
        "# Example 4: Hybrid Azure/On-Premises Strategy\n",
        "problem_hybrid = '''We currently run 35 VMs in Azure and 40 on-premises servers nearing end-of-life.\n",
        "What would be the most cost-effective migration strategy considering TCO for the next 5 years?'''\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "19ed59dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß FinOps Challenge: Our Power BI reports connected to our FinOps hub storage aren't showing the latest data. \n",
            "We're using a service principal to connect to storage. What should we verify to ensure proper data refresh?\n",
            "\n",
            "üí° Solution: To troubleshoot and ensure your Cost Management exports are properly flowing into your storage account for Power BI reporting, follow these steps:\n",
            "\n",
            "### Step-by-Step Troubleshooting and Recommendations\n",
            "\n",
            "#### Step 1: Verify Data Factory Pipeline Configuration\n",
            "1. **Access Azure Data Factory:**\n",
            "   - Go to the Azure portal.\n",
            "   - Navigate to your Data Factory instance.\n",
            "\n",
            "2. **Check Pipeline Runs:**\n",
            "   - In the Data Factory UI, go to the \"Monitor\" tab.\n",
            "   - Review the pipeline runs for any failed activities.\n",
            "   - Click on the failed pipeline run to see detailed error messages.\n",
            "\n",
            "3. **Validate Pipeline Activities:**\n",
            "   - Ensure each activity within the pipeline is correctly configured.\n",
            "   - Check the source dataset (Cost Management exports) and the sink dataset (storage account) configurations.\n",
            "\n",
            "#### Step 2: Validate Cost Management Export Configuration\n",
            "1. **Review Export Settings:**\n",
            "   - Go to the Azure portal.\n",
            "   - Navigate to \"Cost Management + Billing.\"\n",
            "   - Select \"Exports\" under \"Cost Management.\"\n",
            "   - Verify that the export is configured to the correct storage account and container.\n",
            "\n",
            "2. **Check Export Schedule:**\n",
            "   - Ensure the export schedule aligns with your Data Factory pipeline schedule.\n",
            "   - Verify the export frequency (daily, weekly, etc.) and time.\n",
            "\n",
            "#### Step 3: Check Storage Account Permissions\n",
            "1. **Access Storage Account:**\n",
            "   - Go to the Azure portal.\n",
            "   - Navigate to your storage account.\n",
            "\n",
            "2. **Review Access Control (IAM):**\n",
            "   - Ensure the Data Factory managed identity has the necessary permissions (e.g., Storage Blob Data Contributor) on the storage account.\n",
            "\n",
            "3. **Verify Container Access:**\n",
            "   - Check that the container specified in the export configuration exists.\n",
            "   - Ensure there are no access restrictions preventing Data Factory from reading the data.\n",
            "\n",
            "#### Step 4: Validate Data Factory Linked Services\n",
            "1. **Check Linked Services:**\n",
            "   - In the Data Factory UI, go to \"Manage\" -> \"Linked services.\"\n",
            "   - Verify the linked service configurations for both the source (Cost Management export) and sink (storage account).\n",
            "\n",
            "2. **Test Connections:**\n",
            "   - Use the \"Test connection\" button to ensure connectivity between Data Factory and the storage account.\n",
            "\n",
            "#### Step 5: Review Data Factory Activity Logs\n",
            "1. **Access Activity Logs:**\n",
            "   - In the Azure portal, navigate to \"Monitor.\"\n",
            "   - Select \"Activity log\" and filter by Data Factory.\n",
            "\n",
            "2. **Analyze Logs:**\n",
            "   - Look for any errors or warnings related to Data Factory activities.\n",
            "   - Pay attention to any authentication or permission issues.\n",
            "\n",
            "#### Step 6: Ensure Data Factory Pipeline Logic\n",
            "1. **Review Pipeline Logic:**\n",
            "   - Ensure the pipeline logic correctly handles the data flow from the export to the storage account.\n",
            "   - Check for any conditional logic or error handling that might be causing issues.\n",
            "\n",
            "2. **Debug Pipeline:**\n",
            "   - Use the \"Debug\" feature in Data Factory to step through the pipeline execution and identify any issues.\n",
            "\n",
            "### Long-Term Strategies\n",
            "\n",
            "1. **Implement Monitoring and Alerts:**\n",
            "   - Set up monitoring and alerts for Data Factory pipeline runs to proactively identify and address issues.\n",
            "   - Use Azure Monitor to create alerts based on pipeline failures or anomalies.\n",
            "\n",
            "2. **Optimize Data Factory Performance:**\n",
            "   - Review and optimize the performance of your Data Factory pipelines to ensure efficient data processing.\n",
            "   - Consider using parallel processing and optimizing data movement activities.\n",
            "\n",
            "3. **Regularly Review Cost Management Exports:**\n",
            "   - Periodically review and validate the Cost Management export configurations to ensure they remain accurate and aligned with business requirements.\n",
            "\n",
            "4. **Enhance Security and Governance:**\n",
            "   - Implement robust security and governance practices to ensure data integrity and compliance.\n",
            "   - Regularly audit access permissions and configurations.\n",
            "\n",
            "### Business Impact Consideration\n",
            "\n",
            "- **Visibility:** Ensuring accurate and timely data flow into Power BI enhances visibility into cloud spending, enabling better financial management.\n",
            "- **Optimization:** Identifying and resolving data flow issues helps in optimizing cloud costs by providing reliable data for analysis.\n",
            "- **Governance:** Maintaining proper configurations and permissions ensures compliance and reduces the risk of data breaches.\n",
            "\n",
            "### Relevant Microsoft Documentation\n",
            "\n",
            "- [Azure Data Factory Documentation](https://docs.microsoft.com/en-us/azure/data-factory/)\n",
            "- [Cost Management + Billing Documentation](https://docs.microsoft.com/en-us/azure/cost-management-billing/)\n",
            "- [Azure Monitor Documentation](https://docs.microsoft.com/en-us/azure/azure-monitor/)\n",
            "\n",
            "By following these steps and recommendations, you can troubleshoot and ensure that your Cost Management exports are properly flowing into your storage account for Power BI reporting.\n"
          ]
        }
      ],
      "source": [
        "finops_system_prompt = \"\"\"You are a Microsoft FinOps toolkit expert with deep knowledge of:\n",
        "\n",
        "1. FinOps Framework: The operational framework for financial management of cloud resources.\n",
        "2. Microsoft Cost Management: How to analyze, track, and allocate cloud spending.\n",
        "3. FinOps Hub Implementation: Setup, validation, and troubleshooting.\n",
        "4. Data Factory Pipelines: How they ingest and transform cost data.\n",
        "5. Power BI Cost Reports: Connection methods and optimization.\n",
        "6. VM Cost Optimization: Strategies for right-sizing, reservation, and scheduling VMs.\n",
        "\n",
        "When answering questions:\n",
        "- Provide clear, step-by-step guidance\n",
        "- Reference relevant Microsoft documentation\n",
        "- Include both immediate fixes and long-term strategies\n",
        "- Consider the business impact of cost optimization\n",
        "- Balance cost, performance, and security requirements\n",
        "\n",
        "Your goal is to help organizations maximize the business value derived from their cloud investment\n",
        "through the principles of FinOps: Visibility, Optimization, and Governance.\"\"\"\n",
        "\n",
        "def solve_finops_problem(problem):\n",
        "    \"\"\"Solve FinOps-related problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=f\"{problem} Please provide step-by-step analysis with actionable recommendations.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# FinOps optimization examples\n",
        "problem = \"\"\"Our organization recently deployed a FinOps hub but we're seeing errors in our Data Factory pipelines. \n",
        "How can we troubleshoot and ensure our Cost Management exports are properly flowing into our storage account for Power BI reporting?\"\"\"\n",
        "bi_problem = \"\"\"Our Power BI reports connected to our FinOps hub storage aren't showing the latest data. \n",
        "We're using a service principal to connect to storage. What should we verify to ensure proper data refresh?\"\"\"\n",
        "\n",
        "# Example 2: VM Cost Optimization\n",
        "vm_problem = \"\"\"Our Azure environment has 50 D-series and F-series VMs in West Europe running 24/7, \n",
        "but monitoring shows only 60% utilization during business hours and 15% during off-hours.\n",
        "How can we optimize costs while maintaining performance requirements?\"\"\"\n",
        "# Example 3: FinOps Hub Implementation\n",
        "hub_problem = \"\"\"We want to implement the Microsoft FinOps toolkit to get better visibility into our \n",
        "multi-subscription Azure environment. What's the best approach for a mid-sized organization \n",
        "with approximately $25K monthly Azure spend across 12 subscriptions?\"\"\"\n",
        "\n",
        "print(\"üîß FinOps Challenge:\", bi_problem)\n",
        "print(\"\\nüí° Solution:\", solve_finops_problem(problem))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "59620846",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üí∞ VM Cost Analysis: To analyze the cost-efficiency of the Standard_D4s_v3 and Standard_F8s_v2 VMs in West Europe with the given usage pattern, we need to consider several factors: the cost of the VMs, the usage pattern, potential savings from reserved instances, and scheduling opportunities.\n",
            "\n",
            "### Step-by-Step Analysis:\n",
            "\n",
            "1. **Calculate Current Costs:**\n",
            "   - **Standard_D4s_v3:**\n",
            "     - vCPU: 4\n",
            "     - RAM: 16 GB\n",
            "     - Cost per hour (Pay-as-you-go): $0.152 (estimate)\n",
            "   - **Standard_F8s_v2:**\n",
            "     - vCPU: 8\n",
            "     - RAM: 16 GB\n",
            "     - Cost per hour (Pay-as-you-go): $0.192 (estimate)\n",
            "\n",
            "   **Usage Pattern:**\n",
            "   - Business hours (60% of the time): 18 hours per day\n",
            "   - Nights/Weekends (40% of the time): 6 hours per day\n",
            "\n",
            "   **Monthly Usage:**\n",
            "   - Total hours in a month: 30 days * 24 hours = 720 hours\n",
            "   - Business hours: 720 * 60% = 432 hours\n",
            "   - Nights/Weekends: 720 * 40% = 288 hours\n",
            "\n",
            "   **Monthly Cost Calculation:**\n",
            "   - **Standard_D4s_v3:**\n",
            "     - Business hours: 432 hours * $0.152 = $65.66\n",
            "     - Nights/Weekends: 288 hours * $0.152 = $43.78\n",
            "     - Total monthly cost: $65.66 + $43.78 = $109.44\n",
            "   - **Standard_F8s_v2:**\n",
            "     - Business hours: 432 hours * $0.192 = $82.94\n",
            "     - Nights/Weekends: 288 hours * $0.192 = $55.30\n",
            "     - Total monthly cost: $82.94 + $55.30 = $138.24\n",
            "\n",
            "2. **Evaluate Newer Generation Alternatives:**\n",
            "   - Consider newer VM series like the Dsv4 or Fsv2 series, which may offer better performance at a lower cost.\n",
            "\n",
            "   **Example:**\n",
            "   - **Standard_D4s_v4:**\n",
            "     - Cost per hour (Pay-as-you-go): $0.136 (estimate)\n",
            "   - **Standard_F8s_v2 (newer generation):**\n",
            "     - Cost per hour (Pay-as-you-go): $0.168 (estimate)\n",
            "\n",
            "   **Monthly Cost Calculation:**\n",
            "   - **Standard_D4s_v4:**\n",
            "     - Business hours: 432 hours * $0.136 = $58.75\n",
            "     - Nights/Weekends: 288 hours * $0.136 = $39.17\n",
            "     - Total monthly cost: $58.75 + $39.17 = $97.92\n",
            "   - **Standard_F8s_v2 (newer generation):**\n",
            "     - Business hours: 432 hours * $0.168 = $72.58\n",
            "     - Nights/Weekends: 288 hours * $0.168 = $48.38\n",
            "     - Total monthly cost: $72.58 + $48.38 = $120.96\n",
            "\n",
            "3. **Evaluate Reserved Instances:**\n",
            "   - Reserved instances can save up to 72% over pay-as-you-go pricing. Let's consider a 1-year reserved instance.\n",
            "\n",
            "   **Example:**\n",
            "   - **Standard_D4s_v3 Reserved:**\n",
            "     - Cost per hour: $0.090 (estimate)\n",
            "   - **Standard_F8s_v2 Reserved:**\n",
            "     - Cost per hour: $0.115 (estimate)\n",
            "\n",
            "   **Monthly Cost Calculation:**\n",
            "   - **Standard_D4s_v3 Reserved:**\n",
            "     - 720 hours * $0.090 = $64.80\n",
            "   - **Standard_F8s_v2 Reserved:**\n",
            "     - 720 hours * $0.115 = $82.80\n",
            "\n",
            "4. **Implement Scheduling Opportunities:**\n",
            "   - Implement VM scheduling to shut down or deallocate VMs during non-business hours to save costs.\n",
            "\n",
            "   **Example:**\n",
            "   - **Standard_D4s_v3 with Scheduling:**\n",
            "     - Only running during business hours (432 hours):\n",
            "     - 432 hours * $0.152 = $65.66\n",
            "   - **Standard_F8s_v2 with Scheduling:**\n",
            "     - Only running during business hours (432 hours):\n",
            "     - 432 hours * $0.192 = $82.94\n",
            "\n",
            "### Summary of Estimated Monthly Savings:\n",
            "\n",
            "- **Standard_D4s_v3 Current Cost: $109.44**\n",
            "  - Newer Generation (D4s_v4): $97.92 (Savings: $11.52)\n",
            "  - Reserved Instance: $64.80 (Savings: $44.64)\n",
            "  - Scheduling: $65.66 (Savings: $43.78)\n",
            "\n",
            "- **Standard_F8s_v2 Current Cost: $138.24**\n",
            "  - Newer Generation (F8s_v2): $120.96 (Savings: $17.28)\n",
            "  - Reserved Instance: $82.80 (Savings: $55.44)\n",
            "  - Scheduling: $82.94 (Savings: $55.30)\n",
            "\n",
            "### Recommendations:\n",
            "\n",
            "1. **Adopt Newer Generation VMs:**\n",
            "   - Transition to D4s_v4 and F8s_v2 for better performance and lower costs.\n",
            "\n",
            "2. **Purchase Reserved Instances:**\n",
            "   - Evaluate the commitment and purchase 1-year reserved instances for significant cost savings.\n",
            "\n",
            "3. **Implement VM Scheduling:**\n",
            "   - Use Azure Automation or Azure Logic Apps to schedule VMs to run only during business hours.\n",
            "\n",
            "By implementing these strategies, you can achieve substantial cost savings while maintaining performance and optimizing cloud resource utilization. For detailed pricing and more accurate estimates, refer to the [Azure Pricing Calculator](https://azure.microsoft.com/en-us/pricing/calculator/) and the [Azure Reserved VM Instances Pricing](https://azure.microsoft.com/en-us/pricing/reserved-vm-instances/).\n"
          ]
        }
      ],
      "source": [
        "finops_system_prompt = \"\"\"You are a Microsoft FinOps toolkit expert with deep knowledge of:\n",
        "\n",
        "1. FinOps Framework: The operational framework for financial management of cloud resources.\n",
        "2. Microsoft Cost Management: How to analyze, track, and allocate cloud spending.\n",
        "3. FinOps Hub Implementation: Setup, validation, and troubleshooting.\n",
        "4. Data Factory Pipelines: How they ingest and transform cost data.\n",
        "5. Power BI Cost Reports: Connection methods and optimization.\n",
        "6. VM Cost Optimization: Strategies for right-sizing, reservation, and scheduling VMs.\n",
        "\n",
        "When answering questions:\n",
        "- Provide clear, step-by-step guidance\n",
        "- Reference relevant Microsoft documentation\n",
        "- Include both immediate fixes and long-term strategies\n",
        "- Consider the business impact of cost optimization\n",
        "- Balance cost, performance, and security requirements\n",
        "\n",
        "Your goal is to help organizations maximize the business value derived from their cloud investment\n",
        "through the principles of FinOps: Visibility, Optimization, and Governance.\"\"\"\n",
        "\n",
        "\n",
        "def analyze_vm_costs(vm_types, region, usage_pattern):\n",
        "    \"\"\"Analyze Azure VM costs with FinOps best practices\"\"\"\n",
        "    query = f\"\"\"As a FinOps specialist, analyze the cost-efficiency of {vm_types} in {region} with {usage_pattern} usage pattern.\n",
        "    Provide cost optimization recommendations including newer generation alternatives, reserved instances evaluation, \n",
        "    and scheduling opportunities. Include estimated monthly savings.\"\"\"\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=query)\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.5,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Example VM cost analysis\n",
        "vm_analysis = analyze_vm_costs(\n",
        "    \"Standard_D4s_v3 and Standard_F8s_v2\", \n",
        "    \"West Europe\", \n",
        "    \"60% during business hours, 15% during nights/weekends\"\n",
        ")\n",
        "print(\"üí∞ VM Cost Analysis:\", vm_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Problem: How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
            "Consider indexing strategies, hardware requirements, and query optimization.\n",
            "\n",
            "‚öôÔ∏è Solution: Optimizing a PostgreSQL database to handle 10,000 transactions per second involves a multi-faceted approach that includes indexing strategies, hardware requirements, and query optimization. Here‚Äôs a step-by-step guide to achieve this:\n",
            "\n",
            "### Step 1: Indexing Strategies\n",
            "1. **Identify Critical Queries**: Determine the most frequently executed queries and the ones that are performance-critical.\n",
            "2. **Use Appropriate Indexes**:\n",
            "   - **Primary and Foreign Keys**: Ensure that primary and foreign key columns are indexed.\n",
            "   - **Composite Indexes**: For queries that filter on multiple columns, create composite indexes.\n",
            "   - **Partial Indexes**: If only a subset of rows is frequently queried, use partial indexes.\n",
            "   - **GIN/GiST Indexes**: For full-text search or complex data types (like JSONB), use GIN or GiST indexes.\n",
            "3. **Index Maintenance**: Regularly monitor and maintain indexes to avoid bloat. Use `REINDEX` and `VACUUM` commands as needed.\n",
            "\n",
            "### Step 2: Hardware Requirements\n",
            "1. **CPU**: Opt for multi-core CPUs. PostgreSQL can utilize multiple cores for parallel query execution and handling multiple connections.\n",
            "2. **Memory (RAM)**: Ensure sufficient RAM to keep frequently accessed data in memory. Aim for at least 64GB or more, depending on the dataset size.\n",
            "3. **Storage**:\n",
            "   - **SSD**: Use SSDs for faster read/write operations.\n",
            "   - **RAID Configuration**: Consider RAID 10 for a balance of performance and redundancy.\n",
            "4. **Network**: Ensure a high-speed network connection if the database is accessed remotely.\n",
            "\n",
            "### Step 3: Query Optimization\n",
            "1. **Analyze and Tune Queries**: Use `EXPLAIN` and `EXPLAIN ANALYZE` to understand query execution plans and identify bottlenecks.\n",
            "2. **Optimize Joins**: Ensure that joins are performed on indexed columns and that join order is optimal.\n",
            "3. **Avoid Sequential Scans**: Ensure that queries use indexes instead of performing full table scans.\n",
            "4. **Connection Pooling**: Use a connection pooler like PgBouncer to manage database connections efficiently.\n",
            "5. **Prepared Statements**: Use prepared statements for repeated queries to reduce parsing and planning overhead.\n",
            "\n",
            "### Step 4: Configuration Tuning\n",
            "1. **PostgreSQL Configuration**: Adjust PostgreSQL settings in `postgresql.conf`:\n",
            "   - `shared_buffers`: Set to 25-40% of available RAM.\n",
            "   - `work_mem`: Increase for complex queries, but be cautious of total memory usage.\n",
            "   - `maintenance_work_mem`: Increase for maintenance operations like `VACUUM`.\n",
            "   - `checkpoint_completion_target`: Set to 0.7 or higher to spread out checkpoint I/O.\n",
            "   - `wal_buffers`: Increase to handle high transaction rates.\n",
            "   - `max_connections`: Set to a reasonable number and use connection pooling.\n",
            "2. **Autovacuum**: Ensure autovacuum is enabled and tuned to prevent table bloat.\n",
            "\n",
            "### Step 5: Monitoring and Maintenance\n",
            "1. **Monitoring Tools**: Use tools like pg_stat_statements, pgBadger, or third-party monitoring solutions to track performance metrics.\n",
            "2. **Regular Maintenance**: Schedule regular maintenance tasks like `VACUUM`, `ANALYZE`, and `REINDEX`.\n",
            "\n",
            "### Final Answer\n",
            "To optimize a PostgreSQL database handling 10,000 transactions per second, implement the following strategies:\n",
            "\n",
            "1. **Indexing**: Use appropriate indexes (primary, foreign keys, composite, partial, GIN/GiST) and maintain them regularly.\n",
            "2. **Hardware**: Invest in multi-core CPUs, ample RAM (64GB+), SSD storage with RAID 10, and a high-speed network.\n",
            "3. **Query Optimization**: Analyze and tune queries, optimize joins, avoid sequential scans, use connection pooling, and prepared statements.\n",
            "4. **Configuration Tuning**: Adjust PostgreSQL settings (shared_buffers, work_mem, etc.) and ensure autovacuum is properly configured.\n",
            "5. **Monitoring and Maintenance**: Use monitoring tools and perform regular maintenance tasks.\n",
            "\n",
            "{Implementing these strategies will help optimize your PostgreSQL database to handle 10,000 transactions per second effectively.}\n"
          ]
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            UserMessage(content=f\"{problem} Please reason step by step, and put your final answer within \\boxed{{}}.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
        "Consider indexing strategies, hardware requirements, and query optimization.\"\"\"\n",
        "\n",
        "print(\"üîß Problem:\", problem)\n",
        "print(\"\\n‚öôÔ∏è Solution:\", solve_technical_problem(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "054af028",
      "metadata": {},
      "source": [
        "## FINOPS TOOLKIT TECH SUPPORT BOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e0a07253",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Initialization failed: Expecting value: line 1 column 1 (char 0)\n",
            "üîß FinOps Challenge: Our organization recently deployed a FinOps hub but we're seeing errors in our Data Factory pipelines. \n",
            "How can we troubleshoot and ensure our Cost Management exports are properly flowing into our storage account for Power BI reporting?\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 239\u001b[39m\n\u001b[32m    235\u001b[39m hub_problem = \u001b[33m\"\"\"\u001b[39m\u001b[33mOur organization recently deployed a FinOps hub but we\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre seeing errors in our Data Factory pipelines. \u001b[39m\n\u001b[32m    236\u001b[39m \u001b[33mHow can we troubleshoot and ensure our Cost Management exports are properly flowing into our storage account for Power BI reporting?\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîß FinOps Challenge:\u001b[39m\u001b[33m\"\u001b[39m, hub_problem)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí° Solution:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43msolve_finops_problem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhub_problem\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Example 2: VM cost analysis\u001b[39;00m\n\u001b[32m    242\u001b[39m vm_analysis = analyze_vm_costs(\n\u001b[32m    243\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mStandard_D4s_v3 and Standard_F8s_v2\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m    244\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWest Europe\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m    245\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m60\u001b[39m\u001b[38;5;132;01m% d\u001b[39;00m\u001b[33muring business hours, 15\u001b[39m\u001b[38;5;132;01m% d\u001b[39;00m\u001b[33muring nights/weekends\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    246\u001b[39m )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36msolve_finops_problem\u001b[39m\u001b[34m(problem)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msolve_finops_problem\u001b[39m(problem):\n\u001b[32m    136\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Solve FinOps-related problems with structured reasoning\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m            \u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinops_system_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m            \u001b[49m\u001b[43mUserMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproblem\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m Please provide step-by-step analysis with actionable recommendations.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\ai\\inference\\_patch.py:728\u001b[39m, in \u001b[36mChatCompletionsClient.complete\u001b[39m\u001b[34m(self, body, messages, stream, frequency_penalty, presence_penalty, temperature, top_p, max_tokens, response_format, stop, tools, tool_choice, seed, model, model_extras, **kwargs)\u001b[39m\n\u001b[32m    725\u001b[39m _request.url = \u001b[38;5;28mself\u001b[39m._client.format_url(_request.url, **path_format_arguments)\n\u001b[32m    727\u001b[39m _stream = stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m pipeline_response: PipelineResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    732\u001b[39m response = pipeline_response.http_response\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m200\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:240\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m pipeline_request: PipelineRequest[HTTPRequestType] = PipelineRequest(request, context)\n\u001b[32m    239\u001b[39m first_node = \u001b[38;5;28mself\u001b[39m._impl_policies[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m._transport)\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     94\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     98\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     94\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     98\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
            "    \u001b[31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 96 (2 times)]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     94\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     98\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_redirect.py:204\u001b[39m, in \u001b[36mRedirectPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    202\u001b[39m original_domain = get_domain(request.http_request.url) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[33m\"\u001b[39m\u001b[33mallow\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     redirect_location = \u001b[38;5;28mself\u001b[39m.get_redirect_location(response)\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[33m\"\u001b[39m\u001b[33mallow\u001b[39m\u001b[33m\"\u001b[39m]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\policies\\_retry.py:551\u001b[39m, in \u001b[36mRetryPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;28mself\u001b[39m._configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[32m    550\u001b[39m request.context[\u001b[33m\"\u001b[39m\u001b[33mretry_count\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlen\u001b[39m(retry_settings[\u001b[33m\"\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_retry(retry_settings, response):\n\u001b[32m    553\u001b[39m     retry_active = \u001b[38;5;28mself\u001b[39m.increment(retry_settings, response=response)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     94\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     98\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     94\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     98\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
            "    \u001b[31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 96 (3 times)]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:96\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     94\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     98\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\_base.py:128\u001b[39m, in \u001b[36m_TransportRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[32m    119\u001b[39m \n\u001b[32m    120\u001b[39m \u001b[33;03m:param request: The PipelineRequest object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \u001b[33;03m:rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m cleanup_kwargs_for_transport(request.context.options)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PipelineResponse(\n\u001b[32m    127\u001b[39m     request.http_request,\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sender\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    129\u001b[39m     context=request.context,\n\u001b[32m    130\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\azure\\core\\pipeline\\transport\\_requests_basic.py:363\u001b[39m, in \u001b[36mRequestsTransport.send\u001b[39m\u001b[34m(self, request, proxies, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         read_timeout = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mread_timeout\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.connection_config.read_timeout)\n\u001b[32m    362\u001b[39m         timeout = (connection_timeout, read_timeout)\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconnection_verify\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnection_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconnection_cert\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnection_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     response.raw.enforce_content_length = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\ai-foundry-workshop\\venv\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\http\\client.py:1395\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1394\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1395\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1397\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\http\\client.py:325\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\http\\client.py:286\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    288\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    705\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    708\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\ssl.py:1314\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1312\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1313\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python311\\Lib\\ssl.py:1166\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "model_name = os.getenv(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key),\n",
        "        headers={\"x-ms-model-mesh-model-name\": model_name}  # Add the model name in the header\n",
        "    )\n",
        "    print(f\"‚úÖ Client initialized | Model: {client.get_model_info().model_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Initialization failed: {e}\")\n",
        "\n",
        "# Define FinOps system prompt\n",
        "finops_system_prompt = \"\"\"You are a Microsoft FinOps toolkit expert with deep knowledge of:\n",
        "\n",
        "1. FinOps Framework: The operational framework for financial management of cloud resources.\n",
        "2. Microsoft Cost Management: How to analyze, track, and allocate cloud spending.\n",
        "3. FinOps Hub Implementation: Setup, validation, and troubleshooting.\n",
        "4. Data Factory Pipelines: How they ingest and transform cost data.\n",
        "5. Power BI Cost Reports: Connection methods and optimization.\n",
        "6. VM Cost Optimization: Strategies for right-sizing, reservation, and scheduling VMs.\n",
        "\n",
        "When answering questions about FinOps hub validation, include these specific steps:\n",
        "1. Verify Cost Management exports are successful\n",
        "2. Verify Data Factory pipelines are running correctly\n",
        "3. Check msexports container for raw data files\n",
        "4. Check ingestion container for processed parquet files\n",
        "5. Validate Power BI configuration if applicable\n",
        "\n",
        "For troubleshooting Data Factory pipelines, check:\n",
        "- If msexports_ManifestAdded trigger is started\n",
        "- Whether resource providers are registered\n",
        "- For error codes in pipeline runs\n",
        "- For mapping column errors in the ETL pipeline\n",
        "\n",
        "When answering questions:\n",
        "- Provide clear, step-by-step guidance\n",
        "- Reference specific steps from Microsoft's FinOps toolkit documentation\n",
        "- Include both immediate fixes and long-term strategies\n",
        "- Consider the business impact of cost optimization\n",
        "\n",
        "Your goal is to help organizations maximize the business value derived from their cloud investment\n",
        "through the principles of FinOps: Visibility, Optimization, and Governance.\"\"\"\n",
        "\n",
        "# Create the mock documentation function\n",
        "def get_finops_documentation(query):\n",
        "    \"\"\"Mock function to provide FinOps documentation until search is configured\"\"\"\n",
        "    \n",
        "    # Dictionary mapping topics to documentation snippets\n",
        "    finops_docs = {\n",
        "        \"hub\": \"\"\"\n",
        "        # FinOps Hub Troubleshooting Guide\n",
        "        \n",
        "        ## Validate your FinOps hub deployment\n",
        "        \n",
        "        ### Step 1: Verify Cost Management exports\n",
        "        1. Go to Cost Management exports and make sure the export status is 'Successful'.\n",
        "        2. If it isn't successful, ensure you have the Cost Management resource provider registered.\n",
        "        3. File a support request with the Cost Management team to investigate further.\n",
        "        \n",
        "        ### Step 2: Verify Data Factory pipelines\n",
        "        1. From Data Factory Studio, select Monitor and confirm pipelines are running successfully.\n",
        "        2. If pipelines are failing, review the error code and message.\n",
        "        3. Compare the last run time with the time of the last export. They should be close.\n",
        "        4. Select Manage > Author > Triggers and verify the msexports_ManifestAdded trigger is started.\n",
        "        \"\"\",\n",
        "        \n",
        "        \"power bi\": \"\"\"\n",
        "        # Power BI Configuration Troubleshooting\n",
        "        \n",
        "        ## Validate your Power BI configuration\n",
        "        \n",
        "        ### Step 1: Identify your storage URL\n",
        "        Before validation, determine your connection mechanism:\n",
        "        - Cost Management connector for Power BI\n",
        "        - Cost Management exports in storage\n",
        "        - FinOps hubs\n",
        "        \n",
        "        ### Step 2: Connect Power BI to storage\n",
        "        Decide whether to connect using a user/service principal account or storage account key:\n",
        "        \n",
        "        Using a user or service principal account:\n",
        "        1. Ensure you have the Storage Blob Data Reader role explicitly assigned to the account.\n",
        "        \n",
        "        Using a SAS token:\n",
        "        1. Ensure proper permissions: Blob service, Container and Object resource types, Read and List permissions.\n",
        "        2. Ensure valid start and expiry date/time.\n",
        "        \"\"\",\n",
        "        \n",
        "        \"vm\": \"\"\"\n",
        "        # Azure VM Cost Optimization with FinOps\n",
        "        \n",
        "        ## VM Right-sizing Strategies\n",
        "        - Monitor CPU, memory, network and storage metrics to identify underutilized VMs\n",
        "        - Consider resizing to newer-generation instances with better price-performance\n",
        "        - Use Azure Advisor recommendations for right-sizing opportunities\n",
        "        \n",
        "        ## Reserved Instances\n",
        "        - Consider 1-year or 3-year commitments for predictable workloads\n",
        "        - Calculate break-even point to determine optimal reservation term\n",
        "        - Regularly review reservation utilization and exchange or return as needed\n",
        "        \n",
        "        ## Scheduling\n",
        "        - Use Azure Automation or DevTest Labs to automatically stop VMs during off-hours\n",
        "        - Create start/stop schedules that align with business hours for dev/test environments\n",
        "        \"\"\"\n",
        "    }\n",
        "    \n",
        "    # Simple keyword matching\n",
        "    result = []\n",
        "    query = query.lower()\n",
        "    for topic, content in finops_docs.items():\n",
        "        if topic in query or any(keyword in query for keyword in topic.split()):\n",
        "            result.append(content)\n",
        "    \n",
        "    # Default documentation if no specific match\n",
        "    if not result:\n",
        "        result = [finops_docs[\"hub\"]]  # Default to hub documentation\n",
        "        \n",
        "    return \"\\n\\n\".join(result)\n",
        "\n",
        "# Solve general FinOps problems\n",
        "def solve_finops_problem(problem):\n",
        "    \"\"\"Solve FinOps-related problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=f\"{problem} Please provide step-by-step analysis with actionable recommendations.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Function for VM cost analysis\n",
        "def analyze_vm_costs(vm_types, region, usage_pattern):\n",
        "    \"\"\"Analyze Azure VM costs with FinOps best practices\"\"\"\n",
        "    query = f\"\"\"As a FinOps specialist, analyze the cost-efficiency of {vm_types} in {region} with {usage_pattern} usage pattern.\n",
        "    Provide cost optimization recommendations including newer generation alternatives, reserved instances evaluation, \n",
        "    and scheduling opportunities. Include estimated monthly savings.\"\"\"\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=query)\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.5,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Troubleshoot FinOps Hub issues\n",
        "def troubleshoot_finops_hub(issue_description):\n",
        "    \"\"\"Provide troubleshooting steps for FinOps hub issues\"\"\"\n",
        "    \n",
        "    steps_template = \"\"\"Based on the Microsoft FinOps toolkit troubleshooting guide, follow these validation steps:\n",
        "\n",
        "    1. First, check if your issue matches any known error codes\n",
        "    2. Then validate each component in sequence:\n",
        "       - Cost Management exports\n",
        "       - Data Factory pipelines\n",
        "       - Storage account containers (msexports and ingestion)\n",
        "       - Power BI configuration\n",
        "    \n",
        "    For your specific issue with {issue}, I recommend:\"\"\"\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            SystemMessage(content=finops_system_prompt),\n",
        "            UserMessage(content=f\"{steps_template.format(issue=issue_description)} Provide detailed troubleshooting steps.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# FinOps expert function that includes knowledge and documentation\n",
        "def finops_expert(query, include_documentation=True):\n",
        "    \"\"\"Get expert advice on FinOps toolkit questions\"\"\"\n",
        "    \n",
        "    # Optionally retrieve relevant documentation\n",
        "    docs = \"\"\n",
        "    if include_documentation:\n",
        "        docs = get_finops_documentation(query)\n",
        "        docs_context = f\"\\n\\nRelevant Microsoft FinOps documentation:\\n{docs}\\n\\n\"\n",
        "    else:\n",
        "        docs_context = \"\"\n",
        "    \n",
        "    messages = [\n",
        "        SystemMessage(content=finops_system_prompt),\n",
        "        UserMessage(content=f\"{docs_context}User query: {query}\\n\\nProvide expert guidance on this FinOps toolkit question.\")\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        response = client.complete(\n",
        "            messages=messages,\n",
        "            model=model_name,\n",
        "            temperature=0.3,\n",
        "            max_tokens=2048\n",
        "        )\n",
        "        \n",
        "        content = response.choices[0].message.content\n",
        "        \n",
        "        # Extract reasoning if present\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            thinking = match.group(1).strip()\n",
        "            answer = match.group(2).strip()\n",
        "            return {\"thinking\": thinking, \"answer\": answer}\n",
        "        return content\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Example 1: Basic FinOps question\n",
        "hub_problem = \"\"\"Our organization recently deployed a FinOps hub but we're seeing errors in our Data Factory pipelines. \n",
        "How can we troubleshoot and ensure our Cost Management exports are properly flowing into our storage account for Power BI reporting?\"\"\"\n",
        "\n",
        "print(\"üîß FinOps Challenge:\", hub_problem)\n",
        "print(\"\\nüí° Solution:\", solve_finops_problem(hub_problem))\n",
        "\n",
        "# Example 2: VM cost analysis\n",
        "vm_analysis = analyze_vm_costs(\n",
        "    \"Standard_D4s_v3 and Standard_F8s_v2\", \n",
        "    \"West Europe\", \n",
        "    \"60% during business hours, 15% during nights/weekends\"\n",
        ")\n",
        "print(\"\\nüí∞ VM Cost Analysis:\", vm_analysis)\n",
        "\n",
        "# Example 3: FinOps expert with documentation retrieval\n",
        "question = \"How do I validate my FinOps hub deployment?\"\n",
        "print(f\"\\n‚ùì Question: {question}\")\n",
        "result = finops_expert(question)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nüìù Response:\", result)\n",
        "\n",
        "# Example 4: Power BI troubleshooting\n",
        "bi_problem = \"\"\"Our Power BI reports connected to our FinOps hub storage aren't showing the latest data. \n",
        "We're using a service principal to connect to storage. What should we verify to ensure proper data refresh?\"\"\"\n",
        "print(\"\\nüîç Power BI Issue:\", bi_problem)\n",
        "print(\"\\nüîß Troubleshooting Steps:\", troubleshoot_finops_hub(bi_problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "150c569c",
      "metadata": {},
      "source": [
        "# FINOPS TOOLKIT TECHSUPPORT WITH BINGSEARCH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7c47bf55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing DeepSeek-R1 FinOps Toolkit Expert with Tavily Search...\n",
            "‚úÖ Tavily module already installed\n",
            "‚úÖ DeepSeek-R1 client initialized | Model: DeepSeek-R1\n",
            "‚úÖ Tavily client initialized\n",
            "\n",
            "---------- FinOps Expert with Tavily Search ----------\n",
            "\n",
            "üìù Question: How do I validate my FinOps hub deployment?\n",
            "üîç Searching for FinOps documentation about: How do I validate my FinOps hub deployment?\n",
            "‚úÖ Found relevant documentation from Tavily search\n",
            "\n",
            "üîç Answer: <think>\n",
            "Okay, the user is asking how to validate their FinOps hub deployment. Let me start by recalling the key points from the provided documentation and my own knowledge. The Microsoft FinOps documentation mentions that validation involves checking the data factory pipelines and following debugging steps. Also, the FinOps toolkit is open-source and regularly updated, so I should make sure to reference the latest practices.\n",
            "\n",
            "First, I need to outline the validation steps as per the FinOps Framework and the specific checks mentioned in the context. The user needs a step-by-step guide. Let me break it down into the sections mentioned earlier: Cost Management exports, Data Factory pipelines, storage containers, Power BI, and VM optimizations. But since the user's query is specifically about FinOps hub validation, I should focus on the steps related to the hub.\n",
            "\n",
            "From the context, the validation steps include checking Cost Management exports, Data Factory pipelines, msexports container, ingestion container, and Power BI configuration. Also, troubleshooting steps for Data Factory pipelines involve triggers, resource providers, error codes, and ETL mapping.\n",
            "\n",
            "I should structure the answer into numbered steps, each with specific actions. For example, step 1 would be verifying Cost Management exports. The user might need to know where in the Azure portal to go, like navigating to Cost Management, then Exports, and checking the status. Including PowerShell commands could be helpful for automation or script-based checks.\n",
            "\n",
            "Next, checking Data Factory pipelines. The documentation mentions that pipelines should be successful. So the user needs to go to Data Factory, monitor pipeline runs, check for errors. Also, triggers like msexports_ManifestAdded should be active. Maybe there's a specific trigger that starts the pipeline when new data arrives.\n",
            "\n",
            "Then, storage containers: msexports for raw data and ingestion for processed parquet files. The user should check if files are present, maybe with specific naming conventions or dates. They might need to use Azure Storage Explorer or the portal's blob storage interface.\n",
            "\n",
            "Power BI configuration is another step. Ensuring the dataset is connected, scheduled refreshes are working. Maybe mention entering the storage account URL and secret key in Power BI.\n",
            "\n",
            "Troubleshooting tips should include checking triggers, resource providers (like Microsoft.DataFactory registered), error logs in pipeline runs, and column mapping in ETL. Specific error codes might require looking up in Microsoft's documentation, but since the context doesn't list them, I can advise checking the run history details.\n",
            "\n",
            "Also, referencing the provided sources, like the troubleshooting link and the changelog, adds credibility. Mentioning that the toolkit is open-source and updated regularly suggests the user should check the latest docs if issues persist.\n",
            "\n",
            "I need to make sure each step is clear, with portal navigation paths, commands where applicable, and references to the documentation. Avoid jargon, but be precise. For example, \"Navigate to Azure Portal > Cost Management + Billing > Exports\" gives direct steps.\n",
            "\n",
            "Including both immediate fixes (like restarting a trigger) and long-term strategies (monitoring, automation) would be good. Maybe suggest setting up alerts for pipeline failures or using Azure Monitor.\n",
            "\n",
            "Finally, wrap it up by directing the user to official docs for the latest updates, as the toolkit changes frequently. That covers compliance and ensures they're following best practices.\n",
            "</think>\n",
            "\n",
            "Here's a comprehensive validation process for your FinOps hub deployment, combining Microsoft's documentation with operational best practices:\n",
            "\n",
            "**1. Verify Cost Management Exports**\n",
            "```azurecli\n",
            "az costmanagement export list --scope \"subscriptions/<subscription-id>\"\n",
            "```\n",
            "- Portal Path: Azure Portal > Cost Management + Billing > Exports\n",
            "- Check for:\n",
            "  - \"Last successful run\" timestamp <24 hours old\n",
            "  - Status shows \"Healthy\"\n",
            "  - Data available in linked storage account (Storage Account > msexports container)\n",
            "\n",
            "**2. Validate Data Factory Pipelines**\n",
            "```powershell\n",
            "Get-AzDataFactoryV2PipelineRun -ResourceGroupName \"finops-rg\" -DataFactoryName \"adf-finops-core\" -LastUpdatedAfter (Get-Date).AddHours(-24)\n",
            "```\n",
            "- Portal Path: Azure Data Factory > Monitor > Pipeline Runs\n",
            "- Confirm:\n",
            "  - \"FinOps-Hub-Main\" pipeline runs daily\n",
            "  - \"Trigger_FinOps_Orchestration\" is active\n",
            "  - No failed runs in last 7 days (check error details if present)\n",
            "\n",
            "**3. Storage Account Validation**\n",
            "1. **Raw Data (msexports):**\n",
            "   - Check for new CSV files daily in:\n",
            "   `msexports/<subscription-id>/Microsoft.CostManagementExports/<export-name>/`\n",
            "   \n",
            "2. **Processed Data (ingestion):**\n",
            "   - Verify Parquet files exist in:\n",
            "   `ingestion/curated/finops_hub/date=<YYYY-MM-DD>/`\n",
            "\n",
            "**4. Power BI Report Validation**\n",
            "1. In Power BI Service:\n",
            "   - Check dataset refresh history (Last refresh <24h old)\n",
            "   - Verify storage account connection:\n",
            "     ```powerquery\n",
            "     Source = AzureStorage.Blobs(\"https://<storage-account>.blob.core.windows.net/ingestion\")\n",
            "     ```\n",
            "2. Validate report filters:\n",
            "   - Date selector shows current month\n",
            "   - Resource groups dropdown populates correctly\n",
            "\n",
            "**5. Advanced Health Checks**\n",
            "- Trigger verification:\n",
            "  ```azurecli\n",
            "  az datafactory trigger list --factory-name \"adf-finops-core\" --resource-group \"finops-rg\"\n",
            "  ```\n",
            "- Resource provider check:\n",
            "  ```powershell\n",
            "  Get-AzResourceProvider -ProviderNamespace Microsoft.DataFactory\n",
            "  ```\n",
            "- Column mapping validation:\n",
            "  Inspect \"Mapping\" tab in \"Transform_CostData\" data flow for:\n",
            "  - All CSV columns mapped\n",
            "  - No \"Unmapped columns\" warnings\n",
            "\n",
            "**Troubleshooting Tips from Documentation:**\n",
            "1. If pipelines fail:\n",
            "   - Check for \"msexports_ManifestAdded\" trigger status\n",
            "   - Verify Microsoft.DataFactory provider is registered\n",
            "   - Review pipeline error codes against [Microsoft's FinOps error guide](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/help/errors)\n",
            "\n",
            "2. For data freshness issues:\n",
            "   - Check Cost Management export SLA (typically 24-48 hour delay)\n",
            "   - Validate Data Factory schedule trigger configuration\n",
            "\n",
            "**Compliance Check:**\n",
            "As per [Azure compliance documentation](https://azure.microsoft.com/en-us/explore/trusted-cloud/compliance):\n",
            "1. Verify data residency rules are maintained\n",
            "2. Confirm audit logs are enabled for:\n",
            "   - Storage account access\n",
            "   - Data Factory operations\n",
            "   - Power BI dataset refreshes\n",
            "\n",
            "**Recommended Monitoring:**\n",
            "1. Set Azure Monitor alerts for:\n",
            "   - Failed pipeline runs\n",
            "   - Storage account write operations <1/day\n",
            "   - Power BI dataset refresh failures\n",
            "\n",
            "2. Weekly validation checklist:\n",
            "   - Cost data appears in Power BI within 72h\n",
            "   - No persistent \"Unknown\" service names\n",
            "   - VM right-sizing recommendations match actual usage\n",
            "\n",
            "For ongoing validation, implement the FinOps Health Check Workbook available in [FinOps toolkit 0.6+](https://techcommunity.microsoft.com/blog/finopsblog/what%E2%80%99s-new-in-finops-toolkit-0-6-%E2%80%93-september-2024/4266301) that automates these checks through Azure Resource Graph queries.\n",
            "\n",
            "## References\n",
            "1. [Errors](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/help/errors)\n",
            "2. [Finops](https://azure.microsoft.com/en-us/solutions/finops)\n",
            "3. [Changelog](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/changelog)\n",
            "4. [Compliance](https://azure.microsoft.com/en-us/explore/trusted-cloud/compliance)\n",
            "5. [4266301](https://techcommunity.microsoft.com/blog/finopsblog/what%E2%80%99s-new-in-finops-toolkit-0-6-%E2%80%93-september-2024/4266301)\n",
            "\n",
            "\n",
            "üìù Question: What are common issues with Data Factory pipelines in the FinOps hub?\n",
            "üîç Searching for FinOps documentation about: What are common issues with Data Factory pipelines in the FinOps hub?\n",
            "‚úÖ Found relevant documentation from Tavily search\n",
            "\n",
            "üîç Answer: <think>\n",
            "Okay, let's tackle the user's question about common issues with Data Factory pipelines in the FinOps hub. First, I need to recall the documentation provided. The user mentioned several sources, including rerunning activities, private networking, FinOps hub overview, troubleshooting errors, and monitoring with Operations Management Suite.\n",
            "\n",
            "Starting with the common issues: the documentation points out failures due to data timing, code issues, and networking. Also, name resolution is crucial. So, I should break down the answer into these categories. Let me think about each part step by step.\n",
            "\n",
            "First, data arrival issues. The user needs to check if the data is arriving on time. That means verifying the Cost Management exports. Maybe the pipeline is expecting data that hasn't been exported yet. The steps would involve checking the storage account's msexports container. If the files aren't there, the export might not have run. Then, checking the export job in the Azure portal under Cost Management.\n",
            "\n",
            "Next, functional code problems. The user might encounter errors in the pipeline runs. They should look into the pipeline run history for specific error messages. For example, mapping errors in ETL could be due to schema changes. The documentation mentions column mapping errors, so advising them to check the mapping in the copy activity or data flow.\n",
            "\n",
            "Networking issues are another big part. The blog post about securing financial data with private networking mentions name resolution. If components can't resolve each other's names, the pipeline might fail. The user needs to check network peering, routing, and DNS settings. Maybe they're using private endpoints, so ensuring that the private DNS zones are properly linked to the VNet.\n",
            "\n",
            "Also, the triggers. The msexports_ManifestAdded trigger might not be firing. The user should check if the trigger is started and if the blob creation event is properly set up. Maybe the trigger's path is incorrect, or the storage event isn't reaching Data Factory.\n",
            "\n",
            "Monitoring is essential too. The user should set up Azure Data Factory Analytics in Log Analytics to track pipeline health. They can look for failed runs, duration trends, and resource consumption. Alerts can be configured for failures.\n",
            "\n",
            "Resource providers might not be registered. The user should verify that Microsoft.DataFactory and others are registered using Azure CLI or PowerShell. Unregistered providers can cause deployment or runtime issues.\n",
            "\n",
            "For immediate fixes, rerunning the pipeline activity might help, especially if it's a transient issue. But they should also look into the root cause. Long-term strategies include setting up monitoring, automated alerts, and regular validation of networking configurations.\n",
            "\n",
            "Including specific Azure portal steps would help. For example, navigating to Data Factory -> Author & Monitor -> Monitor -> Pipeline runs. Checking the error details there. Also, how to check the storage containers for raw and processed files.\n",
            "\n",
            "PowerShell commands for registering resource providers: Get-AzResourceProvider and Register-AzResourceProvider. Maybe also checking the trigger status with Azure CLI.\n",
            "\n",
            "Referencing the documentation sources where relevant, like the troubleshooting guide and the securing networking blog post.\n",
            "\n",
            "Putting it all together, the answer should list common issues with clear steps, commands, and references. Make sure it's structured so each issue has symptoms, diagnostic steps, and solutions. Also, emphasize both immediate actions and long-term best practices.\n",
            "</think>\n",
            "\n",
            "Here's expert guidance for resolving common Data Factory pipeline issues in FinOps hubs, based on Microsoft's documentation and FinOps best practices:\n",
            "\n",
            "---\n",
            "\n",
            "### **Common Issues & Solutions**\n",
            "\n",
            "#### 1. **Pipeline Failures Due to Late Data Arrival**\n",
            "**Symptoms**: \"Source data not found\" errors in pipeline runs  \n",
            "**Diagnosis**:\n",
            "1. Verify Cost Management exports:\n",
            "   - Azure Portal > Storage Account > **msexports** container\n",
            "   - Check for daily CSV files (Format: `yyyymmdd-<guid>.csv`)\n",
            "   - Validate export configuration:  \n",
            "     `Cost Management > Exports > Verify \"Export status\" = Succeeded`\n",
            "\n",
            "2. Check Data Factory triggers:\n",
            "   - Navigate to:  \n",
            "     `ADF > Author & Monitor > Manage > Triggers`  \n",
            "     Confirm **msexports_ManifestAdded** trigger is in **Started** state\n",
            "\n",
            "**Solution**:  \n",
            "- Rerun failed activities using [Data Factory's rerun capability](https://azure.microsoft.com/en-us/blog/rerun-activities-inside-your-data-factory-pipelines/)  \n",
            "- Set up [Azure Data Factory Analytics](https://azure.microsoft.com/en-us/blog/monitor-azure-data-factory-pipelines-using-operations-management-suite/) for proactive monitoring\n",
            "\n",
            "---\n",
            "\n",
            "#### 2. **Networking/Name Resolution Failures**\n",
            "**Symptoms**: \"Host not found\" or connection timeouts  \n",
            "**Diagnosis**:\n",
            "1. Verify private endpoints:\n",
            "   ```powershell\n",
            "   Get-AzPrivateEndpoint -ResourceGroupName <YourRG>\n",
            "   ```\n",
            "2. Check DNS configuration:\n",
            "   - Ensure Azure Private DNS Zones are linked to VNet  \n",
            "   - Validate name resolution for:  \n",
            "     `{storage}.blob.core.windows.net`  \n",
            "     `{datafactory}.datafactory.azure.net`\n",
            "\n",
            "**Solution**:  \n",
            "[Configure network peering](https://techcommunity.microsoft.com/blog/finopsblog/securing-financial-data-with-private-networking/4370952) with:  \n",
            "1. Azure Portal > Virtual Network > Peerings  \n",
            "2. Validate UDR routes for private endpoints  \n",
            "3. Test connectivity using Data Factory's **Network** tab\n",
            "\n",
            "---\n",
            "\n",
            "#### 3. **ETL Mapping Errors**\n",
            "**Symptoms**: \"Column mismatch\" or schema validation errors  \n",
            "**Diagnosis**:\n",
            "1. Check pipeline run details:  \n",
            "   `ADF > Monitor > Pipeline Runs > Failed Activity > Error Details`  \n",
            "2. Compare source/target schemas:  \n",
            "   - Raw CSV in **msexports** container  \n",
            "   - Processed Parquet in **ingestion** container\n",
            "\n",
            "**Solution**:  \n",
            "1. Update mapping in Data Flow transformations  \n",
            "2. Add schema drift handling:  \n",
            "   ```json\n",
            "   \"script\": \"allowSchemaDrift: true, \n",
            "              validateSchema: false\"\n",
            "   ```\n",
            "\n",
            "---\n",
            "\n",
            "#### 4. **Resource Provider Issues**\n",
            "**Symptoms**: \"Operation not supported\" errors  \n",
            "**Diagnosis**:\n",
            "```azurecli\n",
            "az provider list --query \"[?namespace=='Microsoft.DataFactory']\"\n",
            "```\n",
            "Verify **RegistrationState** = **Registered**\n",
            "\n",
            "**Solution**:\n",
            "```powershell\n",
            "Register-AzResourceProvider -ProviderNamespace Microsoft.DataFactory\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "### **Proactive Monitoring Setup**\n",
            "1. Enable Azure Data Factory Analytics:  \n",
            "   Azure Portal > Marketplace > **Azure Data Factory Analytics**  \n",
            "2. Create alerts for:  \n",
            "   - Pipeline failure rate > 0%  \n",
            "   - Activity duration > SLA threshold  \n",
            "3. Set up weekly validation:  \n",
            "   - Raw data in **msexports**  \n",
            "   - Processed data in **ingestion**  \n",
            "   - Power BI dataset refresh status\n",
            "\n",
            "---\n",
            "\n",
            "### **Documentation References**\n",
            "1. [Rerun Activities Guide](https://azure.microsoft.com/en-us/blog/rerun-activities-inside-your-data-factory-pipelines/)  \n",
            "2. [Private Networking Requirements](https://techcommunity.microsoft.com/blog/finopsblog/securing-financial-data-with-private-networking/4370952)  \n",
            "3. [FinOps Hub Architecture](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/hubs/finops-hubs-overview)\n",
            "\n",
            "**Immediate Action**: Always check pipeline run details first for specific error codes.  \n",
            "**Long-Term Strategy**: Implement the monitoring solution from Step 4 to reduce manual checks by 70% (based on Microsoft's operational data).\n",
            "\n",
            "## References\n",
            "1. [](https://azure.microsoft.com/en-us/blog/rerun-activities-inside-your-data-factory-pipelines/)\n",
            "2. [4370952](https://techcommunity.microsoft.com/blog/finopsblog/securing-financial-data-with-private-networking/4370952)\n",
            "3. [Finops Hubs Overview](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/hubs/finops-hubs-overview)\n",
            "4. [Errors](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/help/errors)\n",
            "5. [](https://azure.microsoft.com/en-us/blog/monitor-azure-data-factory-pipelines-using-operations-management-suite/)\n",
            "\n",
            "\n",
            "üìù Question: How can I troubleshoot Power BI connectivity to my FinOps hub?\n",
            "üîç Searching for FinOps documentation about: How can I troubleshoot Power BI connectivity to my FinOps hub?\n",
            "‚úÖ Found relevant documentation from Tavily search\n",
            "\n",
            "üîç Answer: <think>\n",
            "Okay, the user is asking how to troubleshoot Power BI connectivity to their FinOps hub. Let me start by recalling the key points from the provided documentation. The summary mentions verifying the Azure cost management export and Power BI configuration. The sources also talk about checking data source connections, permissions, and ensuring updates to the latest FOCUS release for Fabric lakehouse support.\n",
            "\n",
            "First, I need to outline the steps systematically. The user might be encountering issues where Power BI can't connect to the Data Lake storage. Common issues could be failed data exports, incorrect Power BI data source configurations, permission issues, or outdated tool versions.\n",
            "\n",
            "From the FinOps documentation, the validation steps include checking Cost Management exports, Data Factory pipelines, storage containers, and Power BI settings. The user's answer should follow these steps. Also, the troubleshooting steps mentioned in the context include verifying triggers, resource providers, error codes, and column mappings in Data Factory.\n",
            "\n",
            "I should structure the answer in a step-by-step manner. Start with verifying the Cost Management export, then check Data Factory pipelines, ensure data is in the correct containers, validate Power BI configuration, check permissions, update to the latest FOCUS version, and consult error documentation.\n",
            "\n",
            "For each step, I need to provide specific actions, like navigating to the Azure portal, checking specific containers, using PowerShell commands if necessary, and referencing the relevant documentation links. Also, mention the upcoming Fabric lakehouse support as a note for future updates.\n",
            "\n",
            "Make sure to include both immediate fixes, like checking pipeline runs, and long-term strategies like updating versions. Also, highlight common errors and where to find solutions in the provided Microsoft Learn links.\n",
            "\n",
            "Finally, ensure the answer is clear, references the documentation, and covers all aspects from data export to Power BI setup, including troubleshooting steps for each component involved in the connectivity.\n",
            "</think>\n",
            "\n",
            "Here's a structured approach to troubleshoot Power BI connectivity with FinOps hub, based on Microsoft's documentation:\n",
            "\n",
            "**Step 1: Verify Cost Management Export Status**\n",
            "1. Navigate to Azure Portal > Cost Management + Billing\n",
            "2. Check \"Exports\" under Cost Management section\n",
            "3. Confirm last successful export matches your billing cycle\n",
            "4. Validate files exist in `msexports` container:\n",
            "   - Go to Storage Account > Containers\n",
            "   - Look for blobs in path: `msexports/tenantId=<your-tenant>/year=<year>/month=<month>`\n",
            "\n",
            "*Documentation Reference: Troubleshoot common errors (Source 4)*\n",
            "\n",
            "**Step 2: Validate Data Factory Pipelines**\n",
            "1. Open Azure Data Factory workspace\n",
            "2. Check \"Pipeline runs\" for errors:\n",
            "   - Verify `msexports_ManifestAdded` trigger is enabled\n",
            "   - Look for failed runs in past 72 hours\n",
            "3. For ETL errors:\n",
            "   - Check column mapping in \"Copy data\" activities\n",
            "   - Verify resource providers are registered:\n",
            "     ```powershell\n",
            "     Register-AzResourceProvider -ProviderNamespace Microsoft.DataFactory\n",
            "     Register-AzResourceProvider -ProviderNamespace Microsoft.Storage\n",
            "     ```\n",
            "\n",
            "*Documentation Reference: FinOps toolkit changelog (Source 5)*\n",
            "\n",
            "**Step 3: Check Processed Data**\n",
            "1. Verify parquet files exist in ingestion container:\n",
            "   - Path: `ingestion/tenantId=<your-tenant>/source=CostManagement/year=<year>/month=<month>`\n",
            "2. Confirm file sizes >0 KB\n",
            "3. Validate file timestamps match pipeline runs\n",
            "\n",
            "**Step 4: Power BI Configuration Check**\n",
            "1. In Power BI Desktop:\n",
            "   - Check data source credentials under \"Data source settings\"\n",
            "   - Verify authentication method matches storage account configuration (Shared Key or Azure AD)\n",
            "2. For lakehouse connections (FOCUS 0.8+):\n",
            "   - Confirm Fabric workspace connection\n",
            "   - Validate Shortcut to Azure Data Lake Storage Gen2 exists\n",
            "\n",
            "**Step 5: Permission Validation**\n",
            "1. Ensure Power BI service principal has:\n",
            "   - Storage Blob Data Reader on ADLS Gen2\n",
            "   - Reader on Data Factory\n",
            "   ```azurecli\n",
            "   az role assignment list --assignee <powerbi-sp-id> --scope /subscriptions/<sub-id>/resourceGroups/<rg-name>\n",
            "   ```\n",
            "\n",
            "**Step 6: Version Compatibility**\n",
            "1. Confirm both FinOps hub and Power BI template use FOCUS 0.8+:\n",
            "   - Check FinOps hub version in Deployment Center\n",
            "   - Update Power BI template from official GitHub repo\n",
            "\n",
            "**Immediate Fixes:**\n",
            "- Reset Power BI credentials: Power BI Service > Datasets > Settings > Data source credentials\n",
            "- Test direct ADLS connection using Storage Explorer\n",
            "\n",
            "**Long-Term Strategies:**\n",
            "1. Enable diagnostic settings on Data Factory for log analytics\n",
            "2. Set up alert rules for failed pipeline runs\n",
            "3. Implement CI/CD for Power BI report updates\n",
            "\n",
            "*Documentation Reference: What's new in 0.8 (Source 2), Troubleshoot errors (Source 4)*\n",
            "\n",
            "If issues persist:\n",
            "1. Check specific error codes against Microsoft's error reference (Source 4)\n",
            "2. Validate network restrictions on storage account\n",
            "3. Test with new Power BI workspace using direct ADLS connection\n",
            "\n",
            "*Note: Fabric lakehouse support requires FOCUS 0.8+ and will be automatically enabled when both FinOps hub and Power BI templates are updated to February 2025 release (Source 2).*\n",
            "\n",
            "## References\n",
            "1. [Most Recent](https://community.fabric.microsoft.com/t5/Issues/idb-p/Issues/status-key/needs_info/tab/most-recent)\n",
            "2. [4391573](https://techcommunity.microsoft.com/blog/finopsblog/what%E2%80%99s-new-in-finops-toolkit-0-8-%E2%80%93-february-2025/4391573)\n",
            "3. [2518223](https://community.fabric.microsoft.com/t5/Desktop/GCP-Billing-amp-FinOps/td-p/2518223)\n",
            "4. [Errors](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/help/errors)\n",
            "5. [Changelog](https://learn.microsoft.com/en-us/cloud-computing/finops/toolkit/changelog)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# FinOps Toolkit Expert with Tavily Web Search Integration\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent.parent.parent  # Going back to the root\n",
        "load_dotenv(parent_dir / '.env')\n",
        "\n",
        "print(\"Initializing DeepSeek-R1 FinOps Toolkit Expert with Tavily Search...\")\n",
        "\n",
        "# Install requirements if needed\n",
        "try:\n",
        "    import tavily\n",
        "    print(\"‚úÖ Tavily module already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing Tavily Python SDK...\")\n",
        "    import pip\n",
        "    pip.main(['install', 'tavily-python'])\n",
        "    import tavily\n",
        "    print(\"‚úÖ Tavily module installed successfully\")\n",
        "\n",
        "try:\n",
        "    # Initialize the DeepSeek-R1 client\n",
        "    endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "    key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "    model_name = os.getenv(\"MODEL_NAME\", \"DeepSeek-R1\")\n",
        "    \n",
        "    # Initialize direct chat client\n",
        "    deepseek_client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key),\n",
        "        headers={\"x-ms-model-mesh-model-name\": model_name}  # Add the model name in the header\n",
        "    )\n",
        "    print(f\"‚úÖ DeepSeek-R1 client initialized | Model: {model_name}\")\n",
        "    \n",
        "    # Initialize Tavily client - you need to set this in your .env file or directly here\n",
        "    # TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\", \"tvly-dev-YOUR_API_KEY_HERE\")\n",
        "    TAVILY_API_KEY = input(\"Please enter your Tavily API key (starts with tvly-): \")\n",
        "    if TAVILY_API_KEY and TAVILY_API_KEY.startswith(\"tvly-\"):\n",
        "        tavily_client = tavily.TavilyClient(TAVILY_API_KEY)\n",
        "        print(\"‚úÖ Tavily client initialized\")\n",
        "    else:\n",
        "        tavily_client = None\n",
        "        print(\"‚ö†Ô∏è Invalid Tavily API key, web search will not be available\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Setup error: {e}\")\n",
        "    deepseek_client = None\n",
        "    tavily_client = None\n",
        "\n",
        "# Function to search for FinOps documentation using Tavily\n",
        "def search_finops_docs_with_tavily(query, max_results=5):\n",
        "    \"\"\"Search for FinOps documentation using Tavily search API\"\"\"\n",
        "    if not tavily_client:\n",
        "        return {\"error\": \"Tavily client not initialized\"}\n",
        "    \n",
        "    # Focus search on Microsoft FinOps documentation\n",
        "    search_query = f\"{query} Microsoft Azure FinOps hub documentation\"\n",
        "    \n",
        "    try:\n",
        "        # Execute the search\n",
        "        response = tavily_client.search(\n",
        "            query=search_query,\n",
        "            search_depth=\"advanced\",  # Use advanced for more comprehensive results\n",
        "            max_results=max_results,\n",
        "            include_answer=\"advanced\",  # Get a summarized answer\n",
        "            include_domains=[\"microsoft.com\", \"azure.com\", \"learn.microsoft.com\"]  # Focus on Microsoft docs\n",
        "        )\n",
        "        \n",
        "        # Format the results in a structured way\n",
        "        formatted_text = \"## Microsoft FinOps Documentation Search Results\\n\\n\"\n",
        "        \n",
        "        # Add the Tavily-generated answer if available\n",
        "        if \"answer\" in response and response[\"answer\"]:\n",
        "            formatted_text += f\"### Summary\\n{response['answer']}\\n\\n\"\n",
        "        \n",
        "        # Add individual search results\n",
        "        if \"results\" in response:\n",
        "            for i, result in enumerate(response[\"results\"], 1):\n",
        "                formatted_text += f\"### {i}. {result.get('title', 'Untitled')}\\n\"\n",
        "                formatted_text += f\"Source: {result.get('url', 'No URL')}\\n\"\n",
        "                formatted_text += f\"{result.get('content', 'No content available')}\\n\\n\"\n",
        "        \n",
        "        return {\n",
        "            \"status\": \"success\",\n",
        "            \"formatted_text\": formatted_text,\n",
        "            \"raw_results\": response[\"results\"] if \"results\" in response else [],\n",
        "            \"urls\": [r.get(\"url\") for r in response.get(\"results\", []) if \"url\" in r],\n",
        "            \"answer\": response.get(\"answer\", \"\")\n",
        "        }\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Tavily search error: {str(e)}\")\n",
        "        return {\n",
        "            \"status\": \"error\",\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Fallback documentation if web search fails\n",
        "def get_finops_documentation(query):\n",
        "    \"\"\"Provide expert FinOps documentation when web search is unavailable\"\"\"\n",
        "    query_lower = query.lower()\n",
        "    \n",
        "    # Choose the appropriate documentation based on query keywords\n",
        "    if any(term in query_lower for term in [\"validate\", \"validation\", \"hub deployment\", \"verify\"]):\n",
        "        return finops_docs[\"hub_validation\"]\n",
        "    elif any(term in query_lower for term in [\"pipeline\", \"data factory\", \"adf\", \"export\"]):\n",
        "        return finops_docs[\"pipeline_issues\"]\n",
        "    elif any(term in query_lower for term in [\"power bi\", \"report\", \"dashboard\", \"visualization\"]):\n",
        "        return finops_docs[\"power_bi\"]\n",
        "    else:\n",
        "        # Default to hub validation\n",
        "        return finops_docs[\"hub_validation\"]\n",
        "\n",
        "# Abbreviated knowledge base - for the full version, use the comprehensive documentation from previous responses\n",
        "finops_docs = {\n",
        "    \"hub_validation\": \"\"\"\n",
        "# FinOps Hub Validation Guide\n",
        "\n",
        "## Step 1: Verify Cost Management exports\n",
        "- Navigate to Cost Management in Azure portal\n",
        "- Check the Exports section to ensure exports are completing successfully\n",
        "- Verify export schedule aligns with your requirements\n",
        "- Check if the export destination (storage account) is accessible\n",
        "\n",
        "## Step 2: Verify Data Factory pipelines\n",
        "- Open Azure Data Factory Studio\n",
        "- Check if the msexports_ManifestAdded trigger is started\n",
        "- Review the pipeline run history for any failures\n",
        "- Examine error details for failing pipeline runs\n",
        "- Verify the msexports_IngestManifestFile pipeline is running correctly\n",
        "\n",
        "## Step 3: Check storage containers\n",
        "- Access the storage account specified in your FinOps hub deployment\n",
        "- Verify the msexports container has export files from Cost Management\n",
        "- Check the ingestion container for processed parquet files\n",
        "- Validate folder structure matches expected patterns\n",
        "\n",
        "## Step 4: Validate Power BI reports\n",
        "- Open Power BI workspace containing FinOps reports\n",
        "- Check dataset refresh history for any failures\n",
        "- Verify service principal has proper access to storage\n",
        "- Test direct connection to storage from Power BI Desktop\n",
        "\"\"\",\n",
        "    \n",
        "    \"pipeline_issues\": \"\"\"\n",
        "# Data Factory Pipeline Troubleshooting\n",
        "\n",
        "## Common Issues:\n",
        "\n",
        "### 1. Permission problems\n",
        "- Ensure Data Factory managed identity has Storage Blob Data Contributor role\n",
        "- Verify managed identity has Reader access to subscription or resource group\n",
        "- Check if Cost Management resource provider is registered\n",
        "\n",
        "### 2. Trigger configuration\n",
        "- Verify msexports_ManifestAdded trigger is started\n",
        "- Check the event trigger is configured for the correct storage account\n",
        "- Validate the trigger path pattern matches your export location\n",
        "\n",
        "### 3. Data format issues\n",
        "- Inspect error logs for column mapping failures in the copy activity\n",
        "- Verify schema definitions match actual export format\n",
        "- Check for special characters or unexpected data types\n",
        "\n",
        "### 4. Networking issues\n",
        "- Confirm storage account firewall allows Azure Data Factory access\n",
        "- Verify private endpoint configurations if using private networking\n",
        "- Check NSG rules if using VNET integration\n",
        "\"\"\",\n",
        "    \n",
        "    \"power_bi\": \"\"\"\n",
        "# Power BI Connection Troubleshooting\n",
        "\n",
        "## Connection Validation Steps:\n",
        "\n",
        "### 1. Authentication verification\n",
        "- Ensure service principal has Storage Blob Data Reader role\n",
        "- Check if SAS token is valid and has appropriate permissions\n",
        "- Verify Azure AD permissions are properly assigned\n",
        "\n",
        "### 2. Connection testing\n",
        "- Test direct connection from Power BI Desktop to storage\n",
        "- Verify container and folder paths in Power BI queries\n",
        "- Check for firewalls blocking Power BI service IP ranges\n",
        "\n",
        "### 3. Gateway configuration\n",
        "- Verify on-premises data gateway if used for hybrid connections\n",
        "- Check gateway connection status in Power BI service\n",
        "- Test credentials used by the gateway\n",
        "\n",
        "### 4. Refresh scheduling\n",
        "- Review dataset refresh history for specific error messages\n",
        "- Verify refresh schedule aligns with data pipeline completion\n",
        "- Check if credential expiration is causing refresh failures\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# FinOps expert function with Tavily search\n",
        "def finops_expert_with_tavily(query):\n",
        "    \"\"\"Get expert FinOps advice using DeepSeek-R1 and Tavily search\"\"\"\n",
        "    \n",
        "    if not deepseek_client:\n",
        "        return \"Error: DeepSeek-R1 client not initialized. Please check your configuration.\"\n",
        "    \n",
        "    try:\n",
        "        print(f\"üîç Searching for FinOps documentation about: {query}\")\n",
        "        \n",
        "        # Try to get documentation from Tavily search if client is available\n",
        "        if tavily_client:\n",
        "            search_result = search_finops_docs_with_tavily(query)\n",
        "            \n",
        "            # Prepare context based on search results\n",
        "            if search_result.get('status') == 'success':\n",
        "                print(\"‚úÖ Found relevant documentation from Tavily search\")\n",
        "                context = search_result['formatted_text']\n",
        "                urls = search_result.get('urls', [])\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Web search failed: {search_result.get('error', 'Unknown error')}\")\n",
        "                print(\"Using built-in FinOps documentation instead\")\n",
        "                context = get_finops_documentation(query)\n",
        "                urls = []\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Tavily client not available, using built-in documentation\")\n",
        "            context = get_finops_documentation(query)\n",
        "            urls = []\n",
        "        \n",
        "        # Define the finops system prompt\n",
        "        finops_system_prompt = \"\"\"You are a Microsoft FinOps toolkit expert with deep knowledge of:\n",
        "        \n",
        "        1. FinOps Framework: The operational framework for financial management of cloud resources.\n",
        "        2. Microsoft Cost Management: How to analyze, track, and allocate cloud spending.\n",
        "        3. FinOps Hub Implementation: Setup, validation, and troubleshooting.\n",
        "        4. Data Factory Pipelines: How they ingest and transform cost data.\n",
        "        5. Power BI Cost Reports: Connection methods and optimization.\n",
        "        6. VM Cost Optimization: Strategies for right-sizing, reservation, and scheduling VMs.\n",
        "        \n",
        "        When answering questions about FinOps hub validation, include these specific steps:\n",
        "        1. Verify Cost Management exports are successful\n",
        "        2. Verify Data Factory pipelines are running correctly\n",
        "        3. Check msexports container for raw data files\n",
        "        4. Check ingestion container for processed parquet files\n",
        "        5. Validate Power BI configuration if applicable\n",
        "        \n",
        "        For Data Factory pipeline troubleshooting, check:\n",
        "        - If msexports_ManifestAdded trigger is started\n",
        "        - Whether all necessary resource providers are registered\n",
        "        - For specific error codes in pipeline run history\n",
        "        - For column mapping errors in the ETL pipeline\n",
        "\n",
        "        When answering questions:\n",
        "        - Provide clear, step-by-step guidance\n",
        "        - Include specific Azure portal navigation steps\n",
        "        - Mention PowerShell or Azure CLI commands where applicable\n",
        "        - Reference the documentation provided in the context\n",
        "        - Include both immediate fixes and long-term strategies\n",
        "        \"\"\"\n",
        "        \n",
        "        # Add the search results as context\n",
        "        user_message = f\"\"\"\n",
        "        Based on Microsoft's FinOps documentation:\n",
        "        \n",
        "        {context}\n",
        "        \n",
        "        User query: {query}\n",
        "        \n",
        "        Please provide expert guidance on this FinOps toolkit question based on the documentation.\n",
        "        Include specific, step-by-step instructions and reference the documentation where relevant.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Get DeepSeek reasoning\n",
        "        response = deepseek_client.complete(\n",
        "            messages=[\n",
        "                SystemMessage(content=finops_system_prompt),\n",
        "                UserMessage(content=user_message)\n",
        "            ],\n",
        "            model=model_name,\n",
        "            temperature=0.3,\n",
        "            max_tokens=2048\n",
        "        )\n",
        "        \n",
        "        content = response.choices[0].message.content\n",
        "        \n",
        "        # Add references to the content if URLs were found\n",
        "        if urls:\n",
        "            content += \"\\n\\n## References\\n\"\n",
        "            for i, url in enumerate(urls, 1):\n",
        "                content += f\"{i}. [{url.split('/')[-1].replace('-', ' ').title()}]({url})\\n\"\n",
        "        \n",
        "        return content\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n---------- FinOps Expert with Tavily Search ----------\")\n",
        "finops_questions = [\n",
        "    \"How do I validate my FinOps hub deployment?\",\n",
        "    \"What are common issues with Data Factory pipelines in the FinOps hub?\",\n",
        "    \"How can I troubleshoot Power BI connectivity to my FinOps hub?\"\n",
        "]\n",
        "\n",
        "# Test with the first question\n",
        "# print(f\"\\nüìù Question: {finops_questions[0]}\")\n",
        "# answer = finops_expert_with_tavily(finops_questions[0])\n",
        "# print(f\"\\nüîç Answer: {answer}\")\n",
        "\n",
        "# Uncomment to test all questions\n",
        "for question in finops_questions:\n",
        "    print(f\"\\nüìù Question: {question}\")\n",
        "    answer = finops_expert_with_tavily(question)\n",
        "    print(f\"\\nüîç Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618292d3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3b9f7a8c",
      "metadata": {},
      "source": [
        "## 4. Best Practices & Considerations\n",
        "\n",
        "1. **Reasoning Handling**: Use regex to separate <think> content from final answers\n",
        "2. **Safety**: Built-in content filtering - handle HttpResponseError for violations\n",
        "3. **Performance**:\n",
        "   - Max tokens: 4096\n",
        "   - Rate limit: 200K tokens/minute\n",
        "4. **Cost**: Pay-as-you-go with serverless deployment\n",
        "5. **Streaming**: Implement response streaming for long completions\n",
        "\n",
        "```python\n",
        "# Streaming example\n",
        "response = client.complete(..., stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "- Leverage 128K context for detailed analysis\n",
        "- Extract reasoning steps for debugging/analysis\n",
        "- Combine with Azure AI Content Safety for production\n",
        "- Monitor token usage via response.usage\n",
        "\n",
        "> Always validate model outputs for critical applications!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
